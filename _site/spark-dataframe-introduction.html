<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>『 Spark 』7. 使用 Spark DataFrame 进行大数据分析 | 云在青天水在瓶</title>
  <meta name="baidu-site-verification" content="6b2f48c1baf35f9e0eb29b4455265203"/>
  <meta name="baidu-site-verification" content="hgXDOPtWLn" />
  <meta name="google-site-verification" content="YqjJD80rZQfugWoznvslaHlII_viwiMiUDEEgPTLEDw" />
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <script src="/files/dc3da690b0d2a5655a8d6150862a2a07.html"></script>
  <!-- <link rel="stylesheet" href="/css/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="/css/default-min.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop-min.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile-min.css" type="text/css" />
  <link rel="shortcut icon" href="/css/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="/css/favicon.ico" mce_href="/favicon.ico" type="image/x-icon">
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.min.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
  <!-- growingIO code -->
  <script type='text/javascript'>
      var _vds = _vds || [];
      window._vds = _vds;
      (function(){
        _vds.push(['setAccountId', '9f3f34627219ccd1']);
        (function() {
          var vds = document.createElement('script');
          vds.type='text/javascript';
          vds.async = true;
          vds.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'dn-growing.qbox.me/vds.js';
          var s = document.getElementsByTagName('script')[0];
          s.parentNode.insertBefore(vds, s);
        })();
      })();
  </script>
  
  <!-- 删掉 baidu spider 主动推送，无效 -->
  <!-- baidu spider initiative push -->
<!-- <script>
    (function(){
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
        }
        else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
  </script> -->
  
  <!-- google analytics push code -->
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-72176628-2', 'auto');
      ga('send', 'pageview');
  </script>

</head>

<!-- meiqia plug-in -->
<!-- 
<script type='text/javascript'>
    (function(m, ei, q, i, a, j, s) {
        m[a] = m[a] || function() {
            (m[a].a = m[a].a || []).push(arguments)
        };
        j = ei.createElement(q),
            s = ei.getElementsByTagName(q)[0];
        j.async = true;
        j.charset = 'UTF-8';
        j.src = i + '?v=' + new Date().getUTCDate();
        s.parentNode.insertBefore(j, s);
    })(window, document, 'script', '//static.meiqia.com/dist/meiqia.js', '_MEIQIA');
    _MEIQIA('entId', 15857);
</script>
 -->

<body>
  <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
  html {
    /*background: #333333;*/
    background: rgb(246, 246, 246);
    -webkit-background-size: cover;
    -moz-background-size: cover;
    -o-background-size: cover;
    background-size: cover;
  }
  /*body { background:transparent;}*/
  @media screen and (max-width: 770px){
    body { background: rgba(255, 255, 255, 0.9); }
  }
</style>

<div id="content" class="post" style="margin-top: 20px;">
  <div id="avatar" class="avatar circle" data-in-right="false" style="width: 150px; height: 150px; position: fixed; top: 40px; z-index: 99; opacity: 0;">
    <div class="center" style="margin-top: 4px; height: 142px; width: 142px; border-radius: 71px; background-image: url('../images/2.jpg');"></div>
  </div>

  <div class="entry" style="position: relative;">
    <h1 class="entry-title"><a href="/spark-dataframe-introduction" title="『 Spark 』7. 使用 Spark DataFrame 进行大数据分析">『 Spark 』7. 使用 Spark DataFrame 进行大数据分析</a></h1>    

    <p class="entry-date">2016-03-30 
        <span class="lastModified" style="display: none;" data-source="_posts/new-spark/2016-03-30-spark-dataframe-introduction.md">最后更新时间: 
        </span>
    </p>


    <h2 id="写在前面">写在前面</h2>

<p>本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。</p>

<p>其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本号还是必要的。 <br />
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。</p>

<p>Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦；3. 点击右边目录上方的 <em>present mode</em> 哦。</p>

<h2 id="1-什么是-spark-dataframe">1. 什么是 spark dataframe</h2>

<p>先来看看官方原汁原味的文档是怎么介绍的：</p>

<p>A DataFrame is <code class="highlighter-rouge">a distributed collection of data</code> organized into named columns. It is conceptually equivalent to a <code class="highlighter-rouge">table in a relational database</code> or a data frame in R/Python, but with <code class="highlighter-rouge">richer optimizations</code> under the hood. DataFrames can be constructed from a wide array of sources such as: <code class="highlighter-rouge">structured data files, tables in Hive, external databases, or existing RDDs</code>.</p>

<p>我们可以看到 spark dataframe 的几个关键点：</p>

<ul>
  <li>分布式的数据集</li>
  <li>类似关系型数据库中的table，或者 excel 里的一张 sheet，或者 python/R 里的 dataframe</li>
  <li>拥有丰富的操作函数，类似于 rdd 中的算子</li>
  <li>一个 dataframe 可以被注册成一张数据表，然后用 sql 语言在上面操作</li>
  <li>丰富的创建方式
    <ul>
      <li>已有的RDD</li>
      <li>结构化数据文件</li>
      <li>JSON数据集</li>
      <li>Hive表</li>
      <li>外部数据库</li>
    </ul>
  </li>
</ul>

<h2 id="2-为什么要用-spark-dataframe">2. 为什么要用 spark dataframe</h2>

<p>为什么要用 dataframe，从细节实现上来说，这个问题比较复杂，不过，基本上下面这张图就能说明所有问题了：</p>

<p><img src="../images/spark-dataframe-flow.png" alt="spark-dataframe-flow.png" /></p>

<p>但是，本文是从基础角度来说 spark dataframe，先不纠结这些细节问题，先了解一些基础的原理和优势，关于上面那张图里面的内容，看后期安排，也许在之后第 15 篇左右会专门讲。</p>

<p>DataFrame API 是在 R 和 Python data frame 的设计灵感之上设计的，具有以下功能特性：</p>

<ul>
  <li>从KB到PB级的数据量支持；</li>
  <li>多种数据格式和多种存储系统支持；</li>
  <li>通过Spark SQL 的 Catalyst优化器进行先进的优化，生成代码；</li>
  <li>通过Spark无缝集成所有大数据工具与基础设施；</li>
  <li>为Python、Java、Scala和R语言（SparkR）API；</li>
</ul>

<p>简单来说，dataframe 能够更方便的操作数据集，而且因为其底层是通过 spark sql 的 Catalyst优化器生成优化后的执行代码，所以其执行速度会更快。总结下来就是，使用 spark dataframe 来构建 spark app，能：</p>

<ul>
  <li><em>write less : 写更少的代码</em></li>
  <li><em>do more : 做更多的事情</em></li>
  <li><em>faster : 以更快的速度</em></li>
</ul>

<h2 id="3-创建-dataframe">3. 创建 dataframe</h2>

<p>因为 spark sql，dataframe，datasets 都是共用 spark sql 这个库的，三者共享同样的代码优化，生成以及执行流程，所以 sql，dataframe，datasets 的入口都是 sqlContext。可用于创建 spark dataframe 的数据源有很多，我们就讲最简单的从结构化文件创建 dataframe。</p>

<p><img src="../images/spark-dataframe-3.jpg" alt="spark-dataframe-3.jpg" /></p>

<ul>
  <li>step 1 : 创建 sqlContext</li>
</ul>

<p>下面是我自己创建 spark sc 都模版：</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">sc_conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span>
<span class="n">sc_conf</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="s">"03-DataFrame-01"</span><span class="p">)</span>
<span class="n">sc_conf</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="n">SPARK_MASTER</span><span class="p">)</span>
<span class="n">sc_conf</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">'spark.executor.memory'</span><span class="p">,</span> <span class="s">'2g'</span><span class="p">)</span>
<span class="n">sc_conf</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">'spark.logConf'</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">sc_conf</span><span class="o">.</span><span class="n">getAll</span><span class="p">()</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">sc_conf</span><span class="p">)</span>
    <span class="n">sqlContext</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span></code></pre></figure>

<ul>
  <li>step 2 : 创建 dataframe，从 json 文件</li>
</ul>

<p>数据文件说明：中国 A 股上市公司基本信息，可以在这里取到：<a href="http://pan.baidu.com/s/1pLxN851">stock_5.json</a></p>

<p><img src="../images/spark-dataframe-1.jpg" alt="spark-dataframe-1.jpg" /></p>

<p>注：这里的 json 文件并不是标准的 json 文件，spark 目前也不支持读取标准的 json 文件。你需要预先把标准的 json 文件处理成 spark 支持的格式: 每一行是一个 json 对象。</p>

<p>比如说，官网的 <code class="highlighter-rouge">people.json</code> 这个例子，它要求的格式是：</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="s2">"name"</span><span class="p">:</span><span class="s2">"Yin"</span><span class="p">,</span><span class="w"> </span><span class="s2">"address"</span><span class="p">:{</span><span class="s2">"city"</span><span class="p">:</span><span class="s2">"Columbus"</span><span class="p">,</span><span class="s2">"state"</span><span class="p">:</span><span class="s2">"Ohio"</span><span class="p">}}</span><span class="w">
</span><span class="p">{</span><span class="s2">"name"</span><span class="p">:</span><span class="s2">"Michael"</span><span class="p">,</span><span class="w"> </span><span class="s2">"address"</span><span class="p">:{</span><span class="s2">"city"</span><span class="p">:</span><span class="kc">null</span><span class="p">,</span><span class="w"> </span><span class="s2">"state"</span><span class="p">:</span><span class="s2">"California"</span><span class="p">}}</span></code></pre></figure>

<p>但对这个文件来看，标准的 json 格式只有下面两种：</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="s2">"name"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"Yin"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Michael"</span><span class="p">],</span><span class="w">
 </span><span class="s2">"address"</span><span class="p">:[</span><span class="w"> 
    </span><span class="p">{</span><span class="s2">"city"</span><span class="p">:</span><span class="s2">"Columbus"</span><span class="p">,</span><span class="s2">"state"</span><span class="p">:</span><span class="s2">"Ohio"</span><span class="p">},</span><span class="w"> 
    </span><span class="p">{</span><span class="s2">"city"</span><span class="p">:</span><span class="kc">null</span><span class="p">,</span><span class="w"> </span><span class="s2">"state"</span><span class="p">:</span><span class="s2">"California"</span><span class="p">}</span><span class="w"> 
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="err">###</span><span class="w"> </span><span class="err">或者</span><span class="w">

</span><span class="p">[</span><span class="w"> 
</span><span class="p">{</span><span class="s2">"name"</span><span class="p">:</span><span class="s2">"Yin"</span><span class="p">,</span><span class="w"> </span><span class="s2">"address"</span><span class="p">:{</span><span class="s2">"city"</span><span class="p">:</span><span class="s2">"Columbus"</span><span class="p">,</span><span class="s2">"state"</span><span class="p">:</span><span class="s2">"Ohio"</span><span class="p">}},</span><span class="w">
</span><span class="p">{</span><span class="s2">"name"</span><span class="p">:</span><span class="s2">"Michael"</span><span class="p">,</span><span class="w"> </span><span class="s2">"address"</span><span class="p">:{</span><span class="s2">"city"</span><span class="p">:</span><span class="kc">null</span><span class="p">,</span><span class="w"> </span><span class="s2">"state"</span><span class="p">:</span><span class="s2">"California"</span><span class="p">}}</span><span class="w">
</span><span class="p">]</span></code></pre></figure>

<p>所以在用 spark sql 来读取一个 json 文件的时候，务必要提前处理好 json 的文件格式，这里我们已经提前处理好了，文件如下所示：</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="s2">"ticker"</span><span class="p">:</span><span class="s2">"000001"</span><span class="p">,</span><span class="s2">"tradeDate"</span><span class="p">:</span><span class="s2">"2016-03-30"</span><span class="p">,</span><span class="s2">"exchangeCD"</span><span class="p">:</span><span class="s2">"XSHE"</span><span class="p">,</span><span class="s2">"secShortName"</span><span class="p">:</span><span class="s2">"</span><span class="se">\u</span><span class="s2">5e73</span><span class="se">\u</span><span class="s2">5b89</span><span class="se">\u</span><span class="s2">94f6</span><span class="se">\u</span><span class="s2">884c"</span><span class="p">,</span><span class="s2">"preClosePrice"</span><span class="p">:</span><span class="mf">10.43</span><span class="p">,</span><span class="s2">"openPrice"</span><span class="p">:</span><span class="mf">10.48</span><span class="p">,</span><span class="s2">"dealAmount"</span><span class="p">:</span><span class="mi">19661</span><span class="p">,</span><span class="s2">"turnoverValue"</span><span class="p">:</span><span class="mf">572627417.1299999952</span><span class="p">,</span><span class="s2">"highestPrice"</span><span class="p">:</span><span class="mf">10.7</span><span class="p">,</span><span class="s2">"lowestPrice"</span><span class="p">:</span><span class="mf">10.47</span><span class="p">,</span><span class="s2">"closePrice"</span><span class="p">:</span><span class="mf">10.7</span><span class="p">,</span><span class="s2">"negMarketValue"</span><span class="p">:</span><span class="mf">126303384220.0</span><span class="p">,</span><span class="s2">"marketValue"</span><span class="p">:</span><span class="mf">153102835340.0</span><span class="p">,</span><span class="s2">"isOpen"</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="s2">"secID"</span><span class="p">:</span><span class="s2">"000001.XSHE"</span><span class="p">,</span><span class="s2">"listDate"</span><span class="p">:</span><span class="s2">"1991-04-03"</span><span class="p">,</span><span class="s2">"ListSector"</span><span class="p">:</span><span class="s2">"</span><span class="se">\u</span><span class="s2">4e3b</span><span class="se">\u</span><span class="s2">677f"</span><span class="p">,</span><span class="s2">"totalShares"</span><span class="p">:</span><span class="mi">14308676200</span><span class="p">},</span><span class="w">
</span><span class="p">{</span><span class="s2">"ticker"</span><span class="p">:</span><span class="s2">"000002"</span><span class="p">,</span><span class="s2">"tradeDate"</span><span class="p">:</span><span class="s2">"2016-03-30"</span><span class="p">,</span><span class="s2">"exchangeCD"</span><span class="p">:</span><span class="s2">"XSHE"</span><span class="p">,</span><span class="s2">"secShortName"</span><span class="p">:</span><span class="s2">"</span><span class="se">\u</span><span class="s2">4e07</span><span class="se">\u</span><span class="s2">79d1A"</span><span class="p">,</span><span class="s2">"preClosePrice"</span><span class="p">:</span><span class="mf">24.43</span><span class="p">,</span><span class="s2">"openPrice"</span><span class="p">:</span><span class="mf">0.0</span><span class="p">,</span><span class="s2">"dealAmount"</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="s2">"turnoverValue"</span><span class="p">:</span><span class="mf">0.0</span><span class="p">,</span><span class="s2">"highestPrice"</span><span class="p">:</span><span class="mf">0.0</span><span class="p">,</span><span class="s2">"lowestPrice"</span><span class="p">:</span><span class="mf">0.0</span><span class="p">,</span><span class="s2">"closePrice"</span><span class="p">:</span><span class="mf">24.43</span><span class="p">,</span><span class="s2">"negMarketValue"</span><span class="p">:</span><span class="mf">237174448154.0</span><span class="p">,</span><span class="s2">"marketValue"</span><span class="p">:</span><span class="mf">269685994760.0</span><span class="p">,</span><span class="s2">"isOpen"</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="s2">"secID"</span><span class="p">:</span><span class="s2">"000002.XSHE"</span><span class="p">,</span><span class="s2">"listDate"</span><span class="p">:</span><span class="s2">"1991-01-29"</span><span class="p">,</span><span class="s2">"ListSector"</span><span class="p">:</span><span class="s2">"</span><span class="se">\u</span><span class="s2">4e3b</span><span class="se">\u</span><span class="s2">677f"</span><span class="p">,</span><span class="s2">"totalShares"</span><span class="p">:</span><span class="mi">11039132000</span><span class="p">}</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">### df is short for dataframe</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s">'hdfs://10.21.208.21:8020/user/mercury/stock_5.json'</span><span class="p">)</span>
<span class="k">print</span> <span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="k">print</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="s">'ticker'</span><span class="p">,</span> <span class="s">'secID'</span><span class="p">,</span> <span class="s">'tradeDate'</span><span class="p">,</span> <span class="s">'listDate'</span><span class="p">,</span> <span class="s">'openPrice'</span><span class="p">,</span> <span class="s">'closePrice'</span><span class="p">,</span> 
                 <span class="s">'highestPrice'</span><span class="p">,</span> <span class="s">'lowestPrice'</span><span class="p">,</span> <span class="s">'isOpen'</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span></code></pre></figure>

<p><img src="../images/spark-dataframe-2.jpg" alt="spark-dataframe-2.jpg" /></p>

<h2 id="4-操作-dataframe">4. 操作 dataframe</h2>

<p>同 rdd 一样，dataframe 也有很多专属于自己的算子，用于操作整个 dataframe 数据集，我们以后都简称为 dataframe api 吧，用 <code class="highlighter-rouge">算子</code>， <code class="highlighter-rouge">DSL</code> 这类的称呼对不熟悉的人来说不易理解，下面这里是完整的 api 列表：<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">spark dataframe api</a></p>

<h3 id="41-在-dataframe-上执行-sql-语句">4.1 在 dataframe 上执行 sql 语句</h3>

<p><img src="../images/spark-dataframe-4.jpg" alt="spark-dataframe-4.jpg" /></p>

<h3 id="42-spark-dataframe-与-pandas-dataframe-转换">4.2 spark dataframe 与 pandas dataframe 转换</h3>

<p>一图胜千言啊：</p>

<p><img src="../images/spark-dataframe-6.jpg" alt="spark-dataframe-6.jpg" /></p>

<p>纵观 spark 的诞生和发展，我觉得 spark 有一点做得非常明智：<em>对同类产品的兼容</em>。从大的方面来说，就像 spark 官网的这段话一样: <em>Runs Everywhere: Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.</em>，spark 对 hadoop 系产品的兼容，让 hadoop 系的开发人员可以轻松的从 hadoop 转到 spark；从小的方面来说，spark 对一些细分工具也照顾 [兼容] 得很好，比如说 spark 推出了 dataframe，人家就可以支持 spark dataframe 和 pandas dataframe 的转换。</p>

<p>熟悉 pandas dataframe 的都了解，pandas 里的 dataframe 可以做很多事情，比如说画图，保存为各种类型的文件，做数据分析什么的。我觉得，可以在 spark 的 dataframe 里做数据处理，分析的整个逻辑，然后可以把最后的结果转化成 pandas 的 dataframe 来展示。当然，如果你的数据量小，也可以直接用 pandas dataframe 来做。</p>

<p><img src="../images/spark-dataframe-7.jpg" alt="spark-dataframe-7.jpg" /></p>

<h2 id="5-一些经验">5. 一些经验</h2>

<h3 id="51-spark-json-格式问题">5.1 spark json 格式问题</h3>

<p>spark 目前也不支持读取标准的 json 文件。你需要预先把标准的 json 文件处理成 spark 支持的格式: 每一行是一个 json 对象。</p>

<h3 id="52-spark-dataframe-和-pandas-dataframe-选择问题">5.2 spark dataframe 和 pandas dataframe 选择问题</h3>

<p>如果数据量小，结构简单，可以直接用 pandas dataframe 来做分析；如果数据量大，结构复杂 [嵌套结构]，那么推荐用 spark dataframe 来做数据分析，然后把结果转成 pandas dataframe，用 pandas dataframe 来做展示和报告。</p>

<h2 id="6-next">6. Next</h2>

<p>ok，dataframe 简单的也说了几句了。我们先缓一缓，上个例子，再接着讲起他的，例子的话就用一个我正在实践的：用 spark 来做量化投资。</p>

<h2 id="7-打开微信扫一扫点一点棒棒的_">7. 打开微信，扫一扫，点一点，棒棒的，^_^</h2>

<p><img src="../images/wechat_pay_6-6.png" alt="wechat_pay_6-6.png" /></p>

<h2 id="参考文章">参考文章</h2>

<ul>
  <li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes">Spark SQL, DataFrames and Datasets Guide</a></li>
  <li><a href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html">Introducing DataFrames in Spark for Large Scale Data Science</a></li>
  <li><a href="https://forums.databricks.com/questions/7257/from-webinar-spark-dataframes-what-is-the-differen-1.html">From Webinar Apache Spark 1.5: What is the difference between a DataFrame and a RDD?</a></li>
  <li><a href="http://www.infoq.com/cn/articles/apache-spark-sql">用Apache Spark进行大数据处理——第二部分：Spark SQL</a></li>
  <li><a href="https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html">An introduction to JSON support in Spark SQL</a></li>
  <li><a href="http://www.csdn.net/article/2015-02-18/2823997">Spark新年福音：一个用于大规模数据科学的API——DataFrame</a></li>
  <li><a href="https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html">An introduction to JSON support in Spark SQL</a></li>
</ul>

<h2 id="本系列文章链接">本系列文章链接</h2>

<ul>
  <li><a href="http://litaotao.github.io/introduction-to-spark?s=inner">『 Spark 』1. spark 简介 </a></li>
  <li><a href="http://litaotao.github.io/spark-questions-concepts?s=inner">『 Spark 』2. spark 基本概念解析 </a></li>
  <li><a href="http://litaotao.github.io/spark-programming-model?s=inner">『 Spark 』3. spark 编程模式 </a></li>
  <li><a href="http://litaotao.github.io/spark-what-is-rdd?s=inner">『 Spark 』4. spark 之 RDD </a></li>
  <li><a href="http://litaotao.github.io/spark-resouces-blogs-paper?s=inner">『 Spark 』5. 这些年，你不能错过的 spark 学习资源 </a></li>
  <li><a href="http://litaotao.github.io/deep-into-spark-exection-model?s=inner">『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task</a></li>
  <li><a href="http://litaotao.github.io/spark-dataframe-introduction?s=inner">『 Spark 』7. 使用 Spark DataFrame 进行大数据分析</a></li>
  <li><a href="http://litaotao.github.io/spark-in-finance-and-investing?s=inner">『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测</a></li>
  <li><a href="http://litaotao.github.io/ipython-notebook-spark?s=inner">『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境</a></li>
  <li><a href="http://litaotao.github.io/boost-spark-application-performance?s=inner">『 Spark 』10. spark 应用程序性能优化｜12 个优化方法</a></li>
  <li><a href="http://litaotao.github.io/spark-mlib-machine-learning?s=inner">『 Spark 』11. spark 机器学习</a></li>
  <li><a href="http://litaotao.github.io/spark-2.0-faster-easier-smarter?s=inner">『 Spark 』12. Spark 2.0 特性介绍</a></li>
  <li><a href="http://litaotao.github.io/spark-2.0-release-notes-zh?s=inner">『 Spark 』13. Spark 2.0 Release Notes 中文版 </a></li>
  <li><a href="http://litaotao.github.io/spark-sql-parquet-optimize?s=inner">『 Spark 』14. 一次 Spark SQL 性能优化之旅</a></li>
</ul>


    <!-- share icon -->
    <!-- <div class="ds-share" data-thread-key="/spark-dataframe-introduction" data-title="『 Spark 』7. 使用 Spark DataFrame 进行大数据分析"
         data-content="content"
         data-url="http://litaotao.github.io//spark-dataframe-introduction">
        <div class="ds-share-aside-left">
          <div class="ds-share-aside-inner">
          </div>
          <div class="ds-share-aside-toggle">分享</div>
        </div>
    </div>
 -->
    <!-- 百度分享按钮 -->

<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"slide":{"type":"slide","bdImg":"7","bdPos":"left","bdTop":"118"},"image":{"viewList":["weixin","qzone","tsina","tqq","renren","sqq","evernotecn","youdao"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["weixin","qzone","tsina","tqq","renren","sqq","evernotecn","youdao"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>


<!--     <div id="disqus_container">
      <div style="margin-bottom:20px">
      多说评论框 start
        <div class="ds-thread" data-thread-key=/spark-dataframe-introduction data-title=『 Spark 』7. 使用 Spark DataFrame 进行大数据分析 
             data-url=http://localhost:4000+/spark-dataframe-introduction></div>
      多说评论框 end
      多说公共JS代码 start (一个网页只需插入一次)
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"litaotao"};
        (function() {
          var ds = document.createElement('script');
          ds.type = 'text/javascript';ds.async = true;
          // ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
          ds.src = '../js/embed.js'
          ds.charset = 'UTF-8';
          (document.getElementsByTagName('head')[0]
           || document.getElementsByTagName('body')[0]).appendChild(ds);
        })();
        </script>
      多说公共JS代码 end
      </div>
    </div> -->

        <div id="disqus_thread"></div>
            <script>

            /**
            *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
            *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
            /*
            var disqus_config = function () {
            this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
            };
            */
            (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://litaotao-github-io.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
            })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>    

  </div>
  
  <div id="menuIndex" class="sidenav">
    <div class="myinfo">
        <center>
          <div id="avatarHolder" class="avatar circle" style="width: 0px; height: 0px; box-shadow: none; margin-bottom: 20px;">
          </div>
          <a href="/index.html" title="Homepage"><i class="icon-home icon-large"></i> Home</a>
          <a href="http://www.linkedin.com/in/taotaoli"><i class="icon-linkedin-sign icon-large"></i> Linkedin</a>
          <a href="https://github.com/zxy"><i class="icon-github icon-large"></i> Code</a>
          <a href="mailto:1757943205@qq.com"><i class="icon-envelope icon-large"></i> Mail</a>
          <button id="present_button" onclick="present_mode()" style="width: 100%; margin-top: 10px; display: none"><i class="icon-align-justify icon-large"></i> Present Mode</button>
        </center>
    </div>
    <div id="menu"></div>
  </div>
</div>

<script src="/js/post.js" type="text/javascript"></script>
<script type="text/javascript">
    //博文页面也做一下刷新操作，避免有时候切换横竖屏时格式不对的问题  
    // $( window ).resize(function() { 
    //     location.reload(); 
    // });
</script>


  <script type="text/javascript">
    var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
    document.write(unescape("%3Cspan id='cnzz_stat_icon_1258855744'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258855744' type='text/javascript'%3E%3C/script%3E"));
  </script>

</body>
</html>
