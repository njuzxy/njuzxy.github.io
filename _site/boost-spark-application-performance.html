<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>『 Spark 』10. spark 应用程序性能优化｜12 个优化方法 | 云在青天水在瓶</title>
  <meta name="baidu-site-verification" content="6b2f48c1baf35f9e0eb29b4455265203"/>
  <meta name="baidu-site-verification" content="hgXDOPtWLn" />
  <meta name="google-site-verification" content="YqjJD80rZQfugWoznvslaHlII_viwiMiUDEEgPTLEDw" />
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <script src="/files/dc3da690b0d2a5655a8d6150862a2a07.html"></script>
  <!-- <link rel="stylesheet" href="/css/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="/css/default-min.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop-min.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile-min.css" type="text/css" />
  <link rel="shortcut icon" href="/css/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="/css/favicon.ico" mce_href="/favicon.ico" type="image/x-icon">
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.min.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
  <!-- growingIO code -->
  <script type='text/javascript'>
      var _vds = _vds || [];
      window._vds = _vds;
      (function(){
        _vds.push(['setAccountId', '9f3f34627219ccd1']);
        (function() {
          var vds = document.createElement('script');
          vds.type='text/javascript';
          vds.async = true;
          vds.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'dn-growing.qbox.me/vds.js';
          var s = document.getElementsByTagName('script')[0];
          s.parentNode.insertBefore(vds, s);
        })();
      })();
  </script>
  
  <!-- 删掉 baidu spider 主动推送，无效 -->
  <!-- baidu spider initiative push -->
<!-- <script>
    (function(){
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
        }
        else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
  </script> -->
  
  <!-- google analytics push code -->
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-72176628-2', 'auto');
      ga('send', 'pageview');
  </script>

</head>

<!-- meiqia plug-in -->
<!-- 
<script type='text/javascript'>
    (function(m, ei, q, i, a, j, s) {
        m[a] = m[a] || function() {
            (m[a].a = m[a].a || []).push(arguments)
        };
        j = ei.createElement(q),
            s = ei.getElementsByTagName(q)[0];
        j.async = true;
        j.charset = 'UTF-8';
        j.src = i + '?v=' + new Date().getUTCDate();
        s.parentNode.insertBefore(j, s);
    })(window, document, 'script', '//static.meiqia.com/dist/meiqia.js', '_MEIQIA');
    _MEIQIA('entId', 15857);
</script>
 -->

<body>
  <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
  html {
    /*background: #333333;*/
    background: rgb(246, 246, 246);
    -webkit-background-size: cover;
    -moz-background-size: cover;
    -o-background-size: cover;
    background-size: cover;
  }
  /*body { background:transparent;}*/
  @media screen and (max-width: 770px){
    body { background: rgba(255, 255, 255, 0.9); }
  }
</style>

<div id="content" class="post" style="margin-top: 20px;">
  <div id="avatar" class="avatar circle" data-in-right="false" style="width: 150px; height: 150px; position: fixed; top: 40px; z-index: 99; opacity: 0;">
    <div class="center" style="margin-top: 4px; height: 142px; width: 142px; border-radius: 71px; background-image: url('../images/2.jpg');"></div>
  </div>

  <div class="entry" style="position: relative;">
    <h1 class="entry-title"><a href="/boost-spark-application-performance" title="『 Spark 』10. spark 应用程序性能优化｜12 个优化方法">『 Spark 』10. spark 应用程序性能优化｜12 个优化方法</a></h1>    

    <p class="entry-date">2016-05-03 
        <span class="lastModified" style="display: none;" data-source="_posts/new-spark/2016-05-03-boost-spark-application-performance.md">最后更新时间: 
        </span>
    </p>


    <h2 id="写在前面">写在前面</h2>

<p>本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。</p>

<p>其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本号还是必要的。 <br />
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。</p>

<p>Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦；3. 点击右边目录上方的 <em>present mode</em> 哦。</p>

<h2 id="1-优化-why-how-when-what">1. 优化? Why? How? When? What?</h2>

<p><img src="../images/how_when_what_why.jpg" alt="how_when_what_why.jpg" /></p>

<p>“spark 应用程序也需要优化？”，很多人可能会有这个疑问，“不是已经有代码生成器，执行优化器，pipeline 什么的了的吗？”。是的，spark 的确是有一些列强大的内置工具，让你的代码在执行时更快。但是，如果一切都依赖于工具，框架来做的话，我想那只能说明两个问题：1. 你对这个框架仅仅是知其然，而非知其所以然；2. 看来你也只是照葫芦画瓢而已，没了你，别人也可以轻轻松松的写这样一个 spark 应用程序，so you are replaceable;</p>

<p>在做 spark 应用程序的优化的时候，从下面几个点出发就够了：</p>

<ul>
  <li>为什么：因为你的资源有限，因为你的应用上生产环境了会有很多不稳定的因素，在上生产前做好优化和测试是唯一一个降低不稳定因素影响的办法；</li>
  <li>怎么做：web ui ＋ log 是做优化的倚天剑和屠龙刀，能掌握好这两点就可以了；</li>
  <li>何时做：应用开发成熟时，满足业务要求时，就可以根据需求和时间安排开始做了；</li>
  <li>做什么：一般来说，spark 应用程序 80% 的优化，都是集中在三个地方：内存，磁盘io，网络io。再细点说，就是 driver，executor 的内存，shuffle 的设置，文件系统的配置，集群的搭建，集群和文件系统的搭建［e.g 尽量让文件系统和集群都在一个局域网内，网络更快；如果可以，可以让 driver 和 集群也在一个局域网内，因为有时候需要从 worker 返回数据到 driver］</li>
  <li>备注：千万不要一心想着优化都从程序本身入手，虽然大多数时候都是程序自己的原因，但在入手检查程序之前最好先确认所有的 worker 机器情况都正常哦。比如说机器负载，网络情况。</li>
</ul>

<p>下面这张图来自 databricks 的一个分享 <a href="https://www.youtube.com/watch?v=kkOG_aJ9KjQ">Tuning and Debugging Apache Spark</a>，很有意思，说得非常对啊，哈哈。</p>

<p><img src="../images/spark-optimization-5.png" alt="spark-optimization-5.png" /></p>

<p>OK，下面我们来看看一些常见的优化方法。</p>

<h2 id="2-repartition-and-coalesce">2. repartition and coalesce</h2>

<p><a href="https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch04.html">原文：</a></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Spark provides the `repartition()` function, which shuffles the data 
across the network to create a new set of partitions. Keep in mind 
that repartitioning your data is a fairly expensive operation. Spark 
also has an optimized version of `repartition()` called `coalesce()` 
that allows avoiding data movement, but only if you are decreasing 
the number of RDD partitions. To know whether you can safely call 
coalesce(), you can check the size of the RDD using `rdd.partitions.size()` 
in Java/Scala and `rdd.getNumPartitions()` in Python and make sure 
that you are coalescing it to fewer partitions than it currently has.
</code></pre></div></div>

<p>总结：当要对 rdd 进行重新分片时，如果目标片区数量小于当前片区数量，那么用 <code class="highlighter-rouge">coalesce</code>，不要用 <code class="highlighter-rouge">repartition</code>。关于 <code class="highlighter-rouge">partition</code> 的更多优化细节，参考 <a href="https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch04.html">chapter 4 of Learning Spark</a></p>

<h2 id="3-passing-functions-to-spark">3. Passing Functions to Spark</h2>

<p>In Python, we have three options for passing functions into Spark.</p>

<ul>
  <li>lambda expressions</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">word</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="s">"error"</span> <span class="ow">in</span> <span class="n">s</span><span class="p">)</span></code></pre></figure>

<ul>
  <li>top-level functions</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">my_personal_lib</span>

<span class="n">word</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">my_personal_lib</span><span class="o">.</span><span class="n">containsError</span><span class="p">)</span></code></pre></figure>

<ul>
  <li>locally defined functions</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">containsError</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">"error"</span> <span class="ow">in</span> <span class="n">s</span>
<span class="n">word</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">containsError</span><span class="p">)</span></code></pre></figure>

<p>One issue to watch out for when passing functions is inadvertently serializing the object containing the function. When you pass a function that is the member of an object, or contains references to fields in an object (e.g., self.field), Spark sends the entire object to worker nodes, which can be much larger than the bit of information you need. Sometimes this can also cause your program to fail, if your class contains objects that Python can’t figure out how to pickle.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">### wrong way</span>

<span class="k">class</span> <span class="nc">SearchFunctions</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">query</span>
  <span class="k">def</span> <span class="nf">isMatch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="ow">in</span> <span class="n">s</span>
  <span class="k">def</span> <span class="nf">getMatchesFunctionReference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
      <span class="c"># Problem: references all of "self" in "self.isMatch"</span>
      <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">isMatch</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">getMatchesMemberReference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
      <span class="c"># Problem: references all of "self" in "self.query"</span>
      <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span>

<span class="c">### the right way</span>

<span class="k">class</span> <span class="nc">WordFunctions</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="o">...</span>
  <span class="k">def</span> <span class="nf">getMatchesNoReference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
      <span class="c"># Safe: extract only the field we need into a local variable</span>
      <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span>
      <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span></code></pre></figure>

<h2 id="4-worker-的资源分配cpu-memroy-executors">4. worker 的资源分配：cpu, memroy, executors</h2>

<p>这个话题比较深，而且在不同的部署模式也不一样 [standalone, yarn, mesos]，这里给不了什么建议。唯一的一个宗旨是，不要一昧考虑把所有资源都独立给到 spark 来用，要考虑到机器本身的一些进程，spark 依赖的一些进程，网络情况，任务情况 [计算密集，IO密集，long-live task]等。</p>

<p>这里只能推荐一些 video，slide 和 blog，具体情况具体分析，以后我遇到资源调优的时候再把实际案例发出来。</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=WyfHUNnMutg">Top 5 Mistakes When Writing Spark Applications</a></li>
</ul>

<h2 id="5-shuffle-block-size-limitation">5. shuffle block size limitation</h2>

<p><em>No Spark shuffle block can be greater than 2 GB</em> — spark shuffle 里的 block size 不能大于 <em>2g</em>。</p>

<p><img src="../images/spark-optimization-1.png" alt="spark-optimization-1.png" /></p>

<p>Spark 使用一个叫 <em>ByteBuffer</em> 的数据结构来作为 shuffle 数据的缓存，但这个 <em>ByteBuffer</em> 默认分配的内存是 2g，所以一旦 shuffle 的数据超过 2g 的时候，shuflle 过程会出错。影响 shuffle 数据大小的因素有以下常见的几个：</p>

<ul>
  <li>partition 的数量，partition 越多，分布到每个 partition 上的数据越少，越不容易导致 shuffle 数据过大;</li>
  <li>数据分布不均匀，一般是 <em>groupByKey</em> 后，存在某几个 key 包含的数据过大，导致该 key 所在的 partition 上数据过大，有可能触发后期 shuflle block 大于 2g;</li>
</ul>

<p>一般解决这类办法都是增加 partition 的数量，<a href="https://www.youtube.com/watch?v=WyfHUNnMutg">Top 5 Mistakes When Writing Spark Applications</a> 这里说可以预计让每个 partition 上的数据为 128MB 左右，仅供参考，还是需要具体场景具体分析，这里只把原理讲清楚就行了，并没有一个完美的规范。</p>

<ul>
  <li>sc.textfile 时指定一个比较大的 partition number</li>
  <li>spark.sql.shuffle.partitions</li>
  <li>rdd.repartition</li>
  <li>rdd.coalesce</li>
</ul>

<p><code class="highlighter-rouge">TIPS</code>:</p>

<p>在 partition 小于 2000 和大于 2000 的两种场景下，Spark 使用不同的数据结构来在 shuffle 时记录相关信息，在 partition 大于 2000 时，会有另一种更高效 [压缩] 的数据结构来存储信息。所以如果你的 partition 没到 2000，但是很接近 2000，可以放心的把 partition 设置为 2000 以上。</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">loc</span><span class="k">:</span> <span class="kt">BlockManagerId</span><span class="o">,</span> <span class="n">uncompressedSizes</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Long</span><span class="o">])</span><span class="k">:</span> <span class="kt">MapStatus</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">uncompressedSizes</span><span class="o">.</span><span class="n">length</span> <span class="o">&gt;</span> <span class="mi">2000</span><span class="o">)</span> <span class="o">{</span>
      <span class="nc">HighlyCompressedMapStatus</span><span class="o">(</span><span class="n">loc</span><span class="o">,</span> <span class="n">uncompressedSizes</span><span class="o">)</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="k">new</span> <span class="nc">CompressedMapStatus</span><span class="o">(</span><span class="n">loc</span><span class="o">,</span> <span class="n">uncompressedSizes</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span></code></pre></figure>

<h2 id="6-level-of-parallel--partition">6. level of parallel － partition</h2>

<p>先来看看一个 stage 里所有 task 运行的一些性能指标，其中的一些说明：</p>

<ul>
  <li><code class="highlighter-rouge">Scheduler Delay</code>: spark 分配 task 所花费的时间</li>
  <li><code class="highlighter-rouge">Executor Computing Time</code>: executor 执行 task 所花费的时间</li>
  <li><code class="highlighter-rouge">Getting Result Time</code>: 获取 task 执行结果所花费的时间</li>
  <li><code class="highlighter-rouge">Result Serialization Time</code>: task 执行结果序列化时间</li>
  <li><code class="highlighter-rouge">Task Deserialization Time</code>: task 反序列化时间</li>
  <li><code class="highlighter-rouge">Shuffle Write Time</code>: shuffle 写数据时间</li>
  <li><code class="highlighter-rouge">Shuffle Read Time</code>: shuffle 读数据所花费时间</li>
</ul>

<p><img src="../images/spark-optimization-7.png" alt="spark-optimization-7.png" /></p>

<p>而这里要说的 <code class="highlighter-rouge">level of parallel</code>，其实大多数情况下都是指 partition 的数量，partition 数量的变化会影响上面几个指标的变动。我们调优的时候，很多时候都会看上面的指标变化情况。当 partition 变化的时候，上面几个指标变动情况如下：</p>

<ul>
  <li>partition 过小［容易引入 data skew 问题］
    <ul>
      <li><code class="highlighter-rouge">Scheduler Delay</code>: 无明显变化</li>
      <li><code class="highlighter-rouge">Executor Computing Time</code>: 不稳定，有大有小，但平均下来比较大</li>
      <li><code class="highlighter-rouge">Getting Result Time</code>: 不稳定，有大有小，但平均下来比较大</li>
      <li><code class="highlighter-rouge">Result Serialization Time</code>: 不稳定，有大有小，但平均下来比较大</li>
      <li><code class="highlighter-rouge">Task Deserialization Time</code>: 不稳定，有大有小，但平均下来比较大</li>
      <li><code class="highlighter-rouge">Shuffle Write Time</code>: 不稳定，有大有小，但平均下来比较大</li>
      <li><code class="highlighter-rouge">Shuffle Read Time</code>: 不稳定，有大有小，但平均下来比较大</li>
    </ul>
  </li>
  <li>partition 过大
    <ul>
      <li><code class="highlighter-rouge">Scheduler Delay</code>: 无明显变化</li>
      <li><code class="highlighter-rouge">Executor Computing Time</code>: 比较稳定，平均下来比较小</li>
      <li><code class="highlighter-rouge">Getting Result Time</code>: 比较稳定，平均下来比较小</li>
      <li><code class="highlighter-rouge">Result Serialization Time</code>: 比较稳定，平均下来比较小</li>
      <li><code class="highlighter-rouge">Task Deserialization Time</code>: 比较稳定，平均下来比较小</li>
      <li><code class="highlighter-rouge">Shuffle Write Time</code>: 比较稳定，平均下来比较小</li>
      <li><code class="highlighter-rouge">Shuffle Read Time</code>: 比较稳定，平均下来比较小</li>
    </ul>
  </li>
</ul>

<p>那应该怎么设置 partition 的数量呢？这里同样也没有专门的公式和规范，一般都在尝试几次后有一个比较优化的结果。但宗旨是：尽量不要导致 data skew 问题，尽量让每一个 task 执行的时间在一段变化不大的区间之内。</p>

<h2 id="7-data-skew">7. data skew</h2>

<p>大多数时候，我们希望的分布式计算带来的好处应该是像下图这样的效果：</p>

<p><img src="../images/spark-optimization-2.png" alt="spark-optimization-2.png" /></p>

<p>但是，有时候，却是下面这种效果，这就是所谓的 data skew。即数据没有被 <code class="highlighter-rouge">大致均匀</code> 的分布到集群中，这样对一个 task 来说，整个 task 的执行时间取决于第一个数据块被处理的时间。在很多分布式系统中，data skew 都是一个很大的问题，比如说分布式缓存，假设有 10 台缓存机器，但有 50% 的数据都落到其中一台机器上，那么当这台机器 down 掉之后，整个缓存的数据就会丢掉一般，缓存命中率至少 [肯定大于] 降低 50%。这也是很多分布式缓存中要引入一致性哈希，要引入 <code class="highlighter-rouge">虚拟节点 vnode</code> 的原因。</p>

<p><img src="../images/spark-optimization-3.png" alt="spark-optimization-3.png" /></p>

<p>一致性哈希原理图：</p>

<p><img src="../images/consistent_hashing_003.jpg" alt="consistent_hashing_003.jpg" /></p>

<p>回到正题，在 spark 中如何解决 data skew 的问题？首先明确这个问题的发生场景和根源：一般来说，都是 (key, value) 型数据中，key 的分布不均匀，这种场景比较常见的方法是把 key 进行 salt 处理 [不知道 salt 中文应该怎么说]，比如说原来有 2 个 key (key1, key2)，并且 key1 对应的数据集很大，而 key2 对应的数据集相对较小，可以把 key 扩张成多个 key (key1-1, key1-2, …, key1-n, key2-1, key2-2, …, key2-m) ，并且保证 <code class="highlighter-rouge">key1-*</code> 对应的数据都是原始 <code class="highlighter-rouge">key1</code> 对应的数据集上划分而来的，<code class="highlighter-rouge">key2-*</code> 上对应的数据都是原始的 <code class="highlighter-rouge">key2</code> 对应的数据集上划分而来。这样之后，我们有 <code class="highlighter-rouge">m+n</code> 个 key，而且每个 key 对应的数据集都相对较小，并行度增加，每个并行程序处理的数据集大小差别不大，可以大大提速并行处理效率。在这两个个分享里都有提到这种方法：</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=WyfHUNnMutg">Top 5 Mistakes When Writing Spark Applications</a></li>
  <li><a href="https://www.youtube.com/watch?v=8hn2KVC8FvA&amp;index=6&amp;list=PL-x35fyliRwiuc6qy9z2erka2VX8LY53x">Sparkling: Speculative Partition of Data for Spark Applications - Peilong Li</a></li>
</ul>

<h2 id="8-avoid-cartesian-operation">8. avoid cartesian operation</h2>

<p><a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cartesian">rdd.cartesian</a> 操作很耗时，特别是当数据集很大的时候，cartesian 的数量级都是平方级增长的，既耗时也耗空间。</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">cartesian</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span></code></pre></figure>

<h2 id="9-avoid-shuffle-when-possible">9. avoid shuffle when possible</h2>

<p><img src="../images/spark-optimization-6.png" alt="spark-optimization-6.png" /></p>

<p>spark 中的 shuffle 默认是把上一个 stage 的数据写到 disk 上，然后下一个 stage 再从 disk 上读取数据。这里的磁盘 IO 会对性能造成很大的影响，特别是数据量大的时候。</p>

<h2 id="10-use-reducebykey-instead-of-groupbykey-when-possible">10. use reduceByKey instead of GroupByKey when possible</h2>

<p><img src="../images/spark-optimization-9.png" alt="spark-optimization-9.png" />
<img src="../images/spark-optimization-10.png" alt="spark-optimization-10.png" /></p>

<h2 id="11-use-treereduce-instead-of-reduce-when-possible">11. use treeReduce instead of reduce when possible</h2>

<p><img src="../images/spark-optimization-4.png" alt="spark-optimization-4.png" /></p>

<h2 id="12-use-kryo-serializer">12. use Kryo serializer</h2>

<p>spark 应用程序中，在对 RDD 进行 shuffle 和 cache 时，数据都是需要被序列化才可以存储的，此时除了 IO 外，数据序列化也可能是应用程序的瓶颈。这里推荐使用 kryo 序列库，在数据序列化时能保证较高的序列化效率。</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">sc_conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span>
<span class="n">sc_conf</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.serializer"</span><span class="p">,</span> <span class="s">"org.apache.spark.serializer.KryoSerializer"</span><span class="p">)</span></code></pre></figure>

<h2 id="13-next">13. Next</h2>

<p>这些都是一些实际实践中的经验和对一些高质量分享的总结［大多数是来自那些高质量分享］，里面可能有说得不完全正确的地方，在未来亲自实践，调试过后会再有一篇性能调试的 blog 的，本篇仅供参考哦。下一次，我们来看看怎么统一部署和配置 spark 的 cluster，那的确几乎来自个人实践经验了。</p>

<h2 id="14-打开微信扫一扫点一点棒棒的_">14. 打开微信，扫一扫，点一点，棒棒的，^_^</h2>

<p><img src="../images/wechat_pay_6-6.png" alt="wechat_pay_6-6.png" /></p>

<h2 id="参考文章">参考文章</h2>

<ul>
  <li><a href="https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch04.html">chapter 4 of Learning Spark</a></li>
  <li><a href="https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch08.html">chapter 8 of Learning Spark</a></li>
  <li><a href="https://www.youtube.com/watch?v=WyfHUNnMutg">Top 5 Mistakes When Writing Spark Applications</a></li>
  <li><a href="https://www.gitbook.com/book/databricks/databricks-spark-knowledge-base/details">Databricks Spark Knowledge Base</a></li>
  <li><a href="https://www.youtube.com/watch?v=8hn2KVC8FvA&amp;index=6&amp;list=PL-x35fyliRwiuc6qy9z2erka2VX8LY53x">Sparkling: Speculative Partition of Data for Spark Applications - Peilong Li</a></li>
  <li><a href="https://datarus.wordpress.com/2015/05/04/fighting-the-skew-in-spark/">Fighting the skew in Spark</a></li>
  <li><a href="https://www.youtube.com/watch?v=kkOG_aJ9KjQ">Tuning and Debugging Apache Spark</a></li>
  <li><a href="http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism">Tuning Spark</a></li>
  <li><a href="https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html">Avoid GroupByKey</a></li>
</ul>

<h2 id="本系列文章链接">本系列文章链接</h2>

<ul>
  <li><a href="http://litaotao.github.io/introduction-to-spark?s=inner">『 Spark 』1. spark 简介 </a></li>
  <li><a href="http://litaotao.github.io/spark-questions-concepts?s=inner">『 Spark 』2. spark 基本概念解析 </a></li>
  <li><a href="http://litaotao.github.io/spark-programming-model?s=inner">『 Spark 』3. spark 编程模式 </a></li>
  <li><a href="http://litaotao.github.io/spark-what-is-rdd?s=inner">『 Spark 』4. spark 之 RDD </a></li>
  <li><a href="http://litaotao.github.io/spark-resouces-blogs-paper?s=inner">『 Spark 』5. 这些年，你不能错过的 spark 学习资源 </a></li>
  <li><a href="http://litaotao.github.io/deep-into-spark-exection-model?s=inner">『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task</a></li>
  <li><a href="http://litaotao.github.io/spark-dataframe-introduction?s=inner">『 Spark 』7. 使用 Spark DataFrame 进行大数据分析</a></li>
  <li><a href="http://litaotao.github.io/spark-in-finance-and-investing?s=inner">『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测</a></li>
  <li><a href="http://litaotao.github.io/ipython-notebook-spark?s=inner">『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境</a></li>
  <li><a href="http://litaotao.github.io/boost-spark-application-performance?s=inner">『 Spark 』10. spark 应用程序性能优化｜12 个优化方法</a></li>
  <li><a href="http://litaotao.github.io/spark-mlib-machine-learning?s=inner">『 Spark 』11. spark 机器学习</a></li>
  <li><a href="http://litaotao.github.io/spark-2.0-faster-easier-smarter?s=inner">『 Spark 』12. Spark 2.0 特性介绍</a></li>
  <li><a href="http://litaotao.github.io/spark-2.0-release-notes-zh?s=inner">『 Spark 』13. Spark 2.0 Release Notes 中文版 </a></li>
  <li><a href="http://litaotao.github.io/spark-sql-parquet-optimize?s=inner">『 Spark 』14. 一次 Spark SQL 性能优化之旅</a></li>
</ul>


    <!-- share icon -->
    <!-- <div class="ds-share" data-thread-key="/boost-spark-application-performance" data-title="『 Spark 』10. spark 应用程序性能优化｜12 个优化方法"
         data-content="content"
         data-url="http://litaotao.github.io//boost-spark-application-performance">
        <div class="ds-share-aside-left">
          <div class="ds-share-aside-inner">
          </div>
          <div class="ds-share-aside-toggle">分享</div>
        </div>
    </div>
 -->
    <!-- 百度分享按钮 -->

<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"slide":{"type":"slide","bdImg":"7","bdPos":"left","bdTop":"118"},"image":{"viewList":["weixin","qzone","tsina","tqq","renren","sqq","evernotecn","youdao"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["weixin","qzone","tsina","tqq","renren","sqq","evernotecn","youdao"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>


<!--     <div id="disqus_container">
      <div style="margin-bottom:20px">
      多说评论框 start
        <div class="ds-thread" data-thread-key=/boost-spark-application-performance data-title=『 Spark 』10. spark 应用程序性能优化｜12 个优化方法 
             data-url=http://localhost:4000+/boost-spark-application-performance></div>
      多说评论框 end
      多说公共JS代码 start (一个网页只需插入一次)
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"litaotao"};
        (function() {
          var ds = document.createElement('script');
          ds.type = 'text/javascript';ds.async = true;
          // ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
          ds.src = '../js/embed.js'
          ds.charset = 'UTF-8';
          (document.getElementsByTagName('head')[0]
           || document.getElementsByTagName('body')[0]).appendChild(ds);
        })();
        </script>
      多说公共JS代码 end
      </div>
    </div> -->

        <div id="disqus_thread"></div>
            <script>

            /**
            *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
            *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
            /*
            var disqus_config = function () {
            this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
            };
            */
            (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://litaotao-github-io.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
            })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>    

  </div>
  
  <div id="menuIndex" class="sidenav">
    <div class="myinfo">
        <center>
          <div id="avatarHolder" class="avatar circle" style="width: 0px; height: 0px; box-shadow: none; margin-bottom: 20px;">
          </div>
          <a href="/index.html" title="Homepage"><i class="icon-home icon-large"></i> Home</a>
          <a href="http://www.linkedin.com/in/taotaoli"><i class="icon-linkedin-sign icon-large"></i> Linkedin</a>
          <a href="https://github.com/zxy"><i class="icon-github icon-large"></i> Code</a>
          <a href="mailto:1757943205@qq.com"><i class="icon-envelope icon-large"></i> Mail</a>
          <button id="present_button" onclick="present_mode()" style="width: 100%; margin-top: 10px; display: none"><i class="icon-align-justify icon-large"></i> Present Mode</button>
        </center>
    </div>
    <div id="menu"></div>
  </div>
</div>

<script src="/js/post.js" type="text/javascript"></script>
<script type="text/javascript">
    //博文页面也做一下刷新操作，避免有时候切换横竖屏时格式不对的问题  
    // $( window ).resize(function() { 
    //     location.reload(); 
    // });
</script>


  <script type="text/javascript">
    var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
    document.write(unescape("%3Cspan id='cnzz_stat_icon_1258855744'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258855744' type='text/javascript'%3E%3C/script%3E"));
  </script>

</body>
</html>
