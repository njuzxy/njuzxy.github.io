# 如何读一本书



- ***reason***: what makes you read that book
- ***what***: what does that book tells, the opinion, the methodology, using the catalog
- ***Answering***: Does the book answer your questions
- ***questions:*** Any questions after reading
- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews
- ***summary***: use your own word to express summary this book



## 1. 2017.10 | 简约至上：交互设计的四原则

- reason
  - 了解，学习，掌握产品设计的思路和方法
  - 觉得使用的一些产品设计上有很多蹩脚的地方，但是不确定是本身设计有问题，还是我个人的使用习惯太奇葩，所以学习一些设计相关的东西来进行验证
  - 为将来创业做打算，在产品设计方便，虽然不必做到精通，但要做到知其然，知其所以然
  - 兴趣，觉得产品设计也是门学问，挺有意思的
- what
  - 讲述了产品设计中 ***简约至上*** 的原则，我一直很赞同这个观点，这也是我读这本书的最大原因之一
  - 从四个方面来讲如何简约化产品：
    - 删除：去掉所有不必要的按钮，直至减到不能再减
    - 组织：按照有意义的标准将按钮分成组
    - 隐藏：把那些不是最重要的按钮安排在隐秘的地方，避免分散用户的注意力
    - 转移：只保留最基本常用的功能，将其他功能转移到屏幕上，从而将功能复杂性进行转移
  - 本书写得特别简介，基本没有废话，原文也是；中文版几乎是在贵阳回上海的高铁上花两小时看完的；英文版也很薄，准备有时间再看看英文版
  - 虽然书薄，但是内容丰富，例子很多，而且配了很多优质插图
- Answering
  - 的确回答了我一些疑问
- questions
  - 从产品设计本身来说，没有什么太大的问题了，但是扩展到创业方面的话，产品设计只是其中一环，在商业模式分析，市场分析，用户分析这些方面还需要加强学习
- reviews
  - 评价都很高，大多都是读书笔记，有推荐其他书：《点石成金》,《启示录》；知乎问题 [哪三本书是产品经理必读书？为什么？](https://www.zhihu.com/question/20752514)， 里面的书也很不错，可以安排时间读一读。
  - 有些很高质量的评价，比如有一个评价讲了 apple 商标的设计：
  - ![](../images/apple.png)
  - ​



## 2. 2017.10 | 乌合之众，大众心理研究

- reason
  - 看到不少人推荐这本书，豆瓣评分也不错
- what
  - 本书讲了个体和群体之间的心理，以及社会的群体心理特征，可以通过目录来看书写脉络
  - 本书在09.30上海飞贵阳的飞机上看完的，不得不说 kindle 是个好东西，哈哈，但是看完后体验不是特别深刻，只觉得在研究消费群体的时候有一些心理行为可以借鉴，但是也不能一下子说出怎么借鉴
  - 核心观点似乎是说群体都是偏 low 的，人才在群体中会渐渐变得平庸，也许也是从另外一个方面来说天才都是孤独的？
  - 第一部分：群体心理
    - 群体的一般特征：如何从心理学上来定义一个群体以及群体的特征
    - 群体的感情和道德观：冲动，易变，急躁，易受暗示，轻信，夸张，单纯，偏执，专横，保守，羊群
    - 群体的观念，推理和想象力：
    - 群体所采取的宗教形式
  - 第二部分：群体的意见与信念
    - 群体的意见和信念中的间接因素：种族，传统，时间，政治和社会制度，教育
    - 群体意见的直接因素：形象，词语和套话，幻觉，经验，理性
    - 群体领袖及其说服的手法：领袖的动员手段有断言，重复和传染
    - 群体的信念和意见的变化范围
  - 第三部分：不同群体的分类及其特点
    - 群体的分类：异质性群体，同质性群体
    - 被称为犯罪群体的群体：
    - 刑事案件的陪审团：
    - 选民群体
    - 议会
- answering
  - 没有带着问题阅读，纯属好奇
- questions
  - 没有问题
- reviews
  - 有人说中译本不好，最好直接看英文版《the crowd》
  - 本书可以对比《思考，快与慢》一起来看




## 3. 2017.10 | Machine Learning in Action

- reason

  - 之前就看过这本书，印象不错，现在想花点时间深入掌握机器学习相关的知识，就准备以本书为主，边看边查边练的方法来深入学习。

- what

  - 原理+代码来讲，可以中英文对照起看，遇到讲解不清楚，不完整的部分直接google，或者看其他书的相关章节，github上找了一个不错的 repo 来做练习，效果不错：https://github.com/litaotao/MachineLearning

- answering

  - 嗯，解答了不少知识点，最大的体会是只有实践了才知道自己究竟知道多少，理解多深

- question

  - 暂无

- reviews

  - 豆瓣评分 8.4，还行，毕竟没有完美的书，只有善用利用书本的人啦。不过可以吸取一些建议，比如后期遇到问题时可以参考周志华的《机器学习》和李航的《统计学习方法》来互补下。

  - 工程其实最重要的是实践，再简单的理论也能通过实践挖掘很多隐藏的知识点和细节问题，所以可以尝试找一些 kaggle 比赛的公开算法来练习练习。

  - 机器学习的东西可深可浅，知识点比较多，自己做了个简单的脑图来辅助总结：https://www.processon.com/diagraming/598bc11de4b02e9a26ee957c

    ​



## 4. 2017.10 | Docker - 从入门到实践

- reason

  - docker 这个技术是个好东西，以后必然有用到的地方
  - 初期准备基于 docker + tensorflow 来搭建深度学习平台，如此更应该认真的了解 docker 了

- what

  - Docker 简介

    > docker 使用 go 语言开发，基于 linux 内核的 cgroup，namespace，aufs，unionfs 等技术对进程举行封装隔离，属于 ***操作系统层面的虚拟化技术***。 由于隔离的进程独立于宿主和其他隔离的进程，因此也称为***容器***， 最初基于 lxc，后来使用自行开发的 libcontainer，后来演化为 runc 和 containerd。
    >
    > docker 与传统虚拟化区别：
    >
    > - 传统虚拟机是先虚拟出一套硬件，再到上面运行一套完整的操作系统，再到该系统上运行应用程序；
    > - docker 是应用程序直接运行于宿主的内核，容器没有自己的内核，也没有硬件虚拟化，轻便快捷；
    >
    > 为什么要使用 docker：
    >
    > - 高效的利用系统资源；
    > - 快速的启动时间；
    > - 一致的运行环境；
    > - 持续交付和部署；
    > - 轻松迁移，维护，扩展；

  - 基本概念

    > image 镜像：相当于一个 root 文件系统，提供了容器运行时所需要的程序，库，资源，配置等，不包括任何动态数据，其内容在构建之后也不会被改变。使用分层存储的架构。
    >
    > container 容器：镜像和容器的关系，类似于程序设计中 ***类*** 和 ***实例*** 的关系，镜像是静态的定义，容器是运行时的实体，容器可以被创建，启动，停止，删除，暂停等。容器的实质是进程，运行于属于自己的命名空间，有自己的 root 文件系统，网络配置等，运气起来类似于一个独立于宿主系统的操作系统一样。
    >
    > 镜像是用分层存储，容器也是如此。每一个容器运行时，是以镜像为基础，在其上面创建一个当期容器的存储层，我们可以称这个容器运行时为读写准备的存储层为 ***容器储存层***。容器储存层的生命周期同容器是一样的，所以 docker 的最佳实践是容器不应该向其储存层写入任何数据，容器储存层要保证物状态化，所有的文件写入操作，都应该使用 ***数据卷 volume***，或者绑定宿主目录，在这些位置写的时候回直接跳过容器储存层。且数据卷的生命周期独立于容器的，可以保证数据不会随着容器关掉而丢失。
    >
    > registry 仓库：集中存储，分发镜像的服务。一个镜像的唯一性标志：`<仓库名>:<标签>`。

  - 使用镜像

    > docker 运行容器前需要本地存在对应的镜像，如果镜像不在本地，就会从镜像仓库下载。
    >
    > 获取镜像：`docker pull [选项] [docker registry 地址]<仓库名>:<标签>`， registry 地址一般是 `<域名/ip>[: 端口号]`，默认是 docker hub；仓库名是两段式名称，一般是 `<用户名>/<软件名>`，对于 docker hub，如果不给出用户名，则默认为 library，也就是官方镜像。
    >
    > 运行镜像：`docker run -it --rm ubuntu:14.04 bash`，docker run 就是运行容器的明亮，-it 只打开一个交互式终端；`--rm` 说容器退出后将其删除；`bash`放在镜像名字后的是命令，这里我们希望有一个交互式的 shell，所以用 bash。最后可以通过 exit 退出这个容器。

  - 列出镜像

    > 可以使用 docker images 来列出已经下载下来的镜像；
    >
    > docker images 中的镜像体积总和并非是所有镜像实际硬盘消耗的大小，由于 docker 镜像是多层存储结构，并且可以继承，复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 docker 使用 union fs，相同的层只需要保存一份即可，因此实际镜像硬盘所占空间很可能比这个列表镜像大小的总和要小得很多。
    >
    > ***虚悬镜像***: 既没有仓库名也没有标签的镜像称为虚悬镜像，docker pull 和 docker build 都可能导致这种现象发生，一般是因为老的镜像被❤新的镜像覆盖掉。
    >
    > ```shell
    > taotao@mac007:~/google_driver/github/litaotao.github.io$docker images
    > REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE
    > <none>                  <none>              de46bac71bad        2 hours ago         1.24GB
    > <none>                  <none>              96fab440b087        2 hours ago         1.24GB
    > <none>                  <none>              88cd00044d2b        2 hours ago         1.24GB
    > <none>                  <none>              f0abe216fb71        2 hours ago         1.24GB
    > <none>                  <none>              707a4e62e6ed        2 hours ago         1.24GB
    > <none>                  <none>              b22f3f2d3fa6        2 hours ago         1.24GB
    > tf/tf                   latest              bcf2d3d815e1        2 hours ago         1.24GB
    > ```
    >
    > 这类镜像可以直接删掉，没有任何价值：`docker rmi $(docker images -q -f dangling=true)`。
    >
    > ***中间层镜像***： docker images -a 可以查看所有镜像，包括中间层镜像，docker images 只显示顶层镜像。中间层镜像即构建顶级镜像中间过程中会用到的镜像。
    >
    > ***镜像筛选***： `docker images <仓库名>:<标签>` 来筛选镜像。

  - 利用 commit 理解镜像构成

    > `docker run --name webserver -d -p 80:80 nginx` 这条命令使用 nginx 镜像启动了一个容器，命名为 webserver，并且映射了 80 端口。如果修改了容器内部的内容，可以通过 `docker diff <容器名>` 来查看具体的改动。当我们运行一个容器的时候，我们做的任何文件修改都会被记录在容器存储层（没有特定写在数据卷或宿主机的时候），此时可以使用 docker commit 来将容器存储层的数据保存下来成为新的镜像。格式为：`docker commit [options] <container id> [<repo name>:<tag>]`。这里和 git 版本控制很想。commit 后可以通过 docker images 看到新生成的镜像。还可以使用 docker history 查看具体镜像的修改记录。`docker history <repo name>:<tag>`。

  - 使用 Dockerfile 来定制镜像

    >dockerfile 是一个文本文件，包含了一条条 ***指令***，每一条指令构建一层，因此每一条指令的内容就是描述该层应当如何构建。
    >
    >- 核心
    >  - `FROM 基础镜像`，每一个镜像都是从一个基础镜像来就行修改定制的，在一个 dockerfile 文件中，from 是必备的，且必须是第一条指令。
    >  - `RUN 命令` 第二个核心的 run 命令，有两种格式。第一种是 shell 模型，即 `run <shell command>`，第二种是 exec 格式的，即 `run [可执行文件，参数1，参数2]`，类似于函数调用。
    >
    >之前说过，dockerfile 中每一个指令都会建立一层，命令的执行过程一般是：新建立一层，在其上执行相关命令，执行完成后 commit 这一层的修改，构成新镜像。而 union fs 是有最大层数限制的，比如 aufs 限制不超过 127 层。所以尽量把相关命令通过 `&&` 连接符连接起来运行。
    >
    >构建镜像通过命令 `docker build [options] <context/url/->` 。docker 运行时分为 docker engine （也就是服务端守护进程），和 docker client。docker engine 提供来一组 rest api，被称为 docker remote api，而 docker 命令这样的 client 则是通过这组 api 和 docker engine 进行交互的，从而完成各种功能。因此表面上看似我们在本机执行各种 docker 功能，实际上一切都是使用远程调用的形式在 docker engine 端完成，即 C/S 架构，这让我们操作远程 docker engine 变得轻而易举。然后当我们进行镜像构建的时候，并非所有的定制都通过 run 命令进行，经常需要将本地一些文件复制到镜像种，比如通过 copy，add 命令等。而 docker build 镜像时，其实并非在本地构建，而是在 engine 端，所以考虑如何让 engine 和一些本地文件进行交互的时候，就引入了 ***上下文*** 的概念，或者简单的说，上下文是指在构建镜像的时候，会将上下文路径下的所有内容打包上传给 engine 端，engine 收到这个上下文包后，展开就会获得构建镜像所需要的一切文件。

  - dockerfile指令详解

    > copy 复制文件：`copy <source> <destination>`，目标路径可以是容器内的绝对路径，也可以是相对于工作目录第相对路径，工作目录通过 `WORKDIR` 来制定。使用 copy 命令，源文件的各种元数据都会保留，比如读写执行权限，文件变更时间等。
    >
    > add 高级复制：add 语法同 copy，一般来说，所有的文件复制均使用 copy 指令，仅在需要自动解压的场合使用 add。
    >
    > cmd 容器启动命令：cmd 和 run 命令类似，也支持 shell 和 exec 两种格式。cmd 指令用于容器启动的时候制定运行的程序和参数。docker 不是虚拟机，容器中的应用都应该以前台运行，而不是像虚拟机，物理机那样用 upstart/systemd 去启动后台服务，容器内没有后台服务的概念。所以如果这样写 `cmd service nginx start` 的时候，会发现容器执行后就立即退出了。对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义。而使用 `service nginx start`，会被docker解析成 `cmd service nginx start`, 然后解析成 `cmd ['sh', '-c', 'service nginx start']`，因此主进程实际上是 sh，那么当 service nginx start 结束后，sh 也就结束了，sh 作为主进程退出了，自然就会让容器也退出。正确的做法应该是直接执行 nginx 文件，并且要求其以前台形式运行，比如`cmd ["nginx", "-g", "daemon off"]`。
    >
    > ​
    >
    > entrypoint 入口点： 高级 cmd，以后用到的话再看。
    >
    > ​
    >
    > env设置环境变量 ：两种格式：`env <key><value> or env <key>=<value> <key>=<value>`。
    >
    > ​
    >
    > arg 构建参数：格式：`arg <参数名>[=<默认值>]`。效果和 env 一样，都是设置环境变量值，所不同的是 arg 设置的是构建环境的环境变量，在将来容器运行时不会存在这些环境变量的。
    >
    > ​
    >
    > volume 定义匿名卷：格式为：`volume ["path1", "path2", ...] or volume <path>`。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 dockerfile 中我们可以实现制定某些目录挂载为匿名卷，这样运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器储存层写入大量数据。 
    >
    > ​
    >
    > expose 暴露端口：格式为：`expose <端口1>[<端口2, ...]`， expose声明容器提供服务的端口，在 dockerfile 中写入这个有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，另一个是在运行时使用随机端口映射时，也就是 `docker run -P` 时，会自动随机映射到 expose 暴露的端口。要将 expose 和运行时使用 `-P <宿主端口>:<容器端口>` 区分开来。`-p` 是映射宿主端口和容器端口，即将容器的对应端口服务公开给外界访问，而 expose 仅仅声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。
    >
    > ​
    >
    > workdir 指定工作目录：格式为：`workdir <工作目录路径>`。 
    >
    > ​
    >
    > user 指定当前用户：格式：`USER <用户名>`。 user 和 workdir 相似，都是改变环境状态并影响以后的层，workdir 改变工作目录，user 则改变之后层执行的 run，cmd 以及 entrypoint 这类命令的身份。
    >
    > ​
    >
    > healthcheck 健康检查：格式：`healthcheck [options] cmd <cmmand>`, 设置容器检查健康状况的命令； ` healthcheck none` 如果基础镜像有健康检查的指令，使用这行可以屏蔽掉其健康检查指令。
    >
    > ​
    >
    > onbuild 为他人做嫁衣：格式：`onbuild <other command>`， onbuild 是一个特殊的指令，他后面跟的其他指令，比如 run，copy 等，而这些指令在当前镜像构建时不会被执行，只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。

  - 删除本地镜像：

    > `docker rmi [options] <image1> [<image2>, ...]`，其中 image 可以通过镜像 ID，镜像名或者镜像摘要来指定，  `docker rm`  是删除容器。一些高级用法：
    >
    > `docker rmi $(docker images -q -f dangling=true)` ，批量删除虚悬镜像；
    >
    > `docker rmi $(docker images -q redis)`，批量删除所有仓库名为 redis 的镜像；
    >
    > `docker rmi $(docker images -q -f before=mongo:3.2)`，批量删除所有在mongo:3.2 之前的镜像；

  - 镜像的实现原理

    > docker 的每个镜像都由很多层次构成，它使用 union fs 将这些不同的层结合到一个镜像中去。通常union fs 有两个用途，一方面可以实现不借助 lvm，raid 等将多个 disk 挂载到同一个目录下面，另一个更常用的是将一个只读的分支和一个可写的分支联合在一起。

  - 操作容器

    > ***启动***：有两种方式启动容器，一是基于镜像新建容器并启动，二是将终止状态的容器启动。主要命令就是 `docker run`。例如
    >
    > `docker run ubuntu:14.04 /bin/echo "hello world"` : 跟在本地执行 `/bin/echo "hello world"` 没啥区别；
    >
    > `docker run -it ubuntu:14.04 /bin/bash`：启动一个容器并打开一个 bash 终端，允许用户进行交互。其中 -t 是让 docker 分配一个伪终端并绑定到容器的标准输入上；-i 则是让容器的标准输入保持打开。
    >
    > 当利用 docker run 来创建容器时，docker 在后台运行的标准操作包括：
    >
    > - 检查本地是否存在指定的镜像，不存在就从公有仓库下载；
    > - 利用镜像创建并启动一个容器
    > - 分配一个文件系统，并且在只读的镜像层外面挂载一层可读写层
    > - 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去；
    > - 从地址池配置一个 ip 地址给容器
    > - 执行用户指定的应用程序
    > - 执行完毕后容器被终止
    >
    > ***守护态运行***： 使用 -d 可以指定容器在后台运行，并可以使用 docker logs 查看容器的标准输出。
    >
    > ***终止容器***： 使用 docker stop 来终止一个运行中的容器。终止状态的容器可以使用 `docker ps -a` 来查看，处于终止状态的容器，可以通过 `docker start` 命令来重启。
    >
    > ***进入容器***： 使用 `-d` 参数使容器在后台运行时，可以通过 `docker attach` 或者 `nsenter` 工具来进入容器。但是使用 docker attach 有时候不方便，当多个窗口同时 attach 到同一个容器的时候，所有窗口都会同步显示，当某个窗口因命令阻塞时，其他窗口也就无法执行操作了。更好的方法是通过 `docker exec` 来进入容器。

  - 访问仓库

    > ***登陆***： 通过 docker login 来注册登录，注册完成后，本地家目录下的 `.dockercfg` 中会保存用户的认证信息；
    >
    > `docker search ` 可以用来查找官方镜像库；
    >
    > `docker pull` 可以用来下载镜像到本地；

  - 数据管理

    > ***数据卷***： 数据卷是一个可供一个或多个容器使用的特殊目录，类似于 linux 下对文件或者目录进行 mount，它绕过 ufs，可以提供很多有用的特性：
    >
    > - 数据卷可以在容器之间共享和重用；
    > - 对数据卷的修改立即生效；
    > - 对数据卷的更新不会影响镜像；
    > - 数据卷默认会一直存在，即使容器被删除；
    >
    > 通过 `-v` 来创建一个数据卷并挂载到容器里。`docker run -d -P --name web -v /webapp training/webapp python app.py` 里面就创建了一个数据卷到容器的 webapp 目录，也可以在 dockerfile 中通过关键字 volume 来添加一个或多个新的卷到由该镜像创建的任意容器。
    >
    > 数据卷是用来持久化数据的，它的生命周期独立于容器。如果在删除容器的同时要移除数据卷，可以在删除容器的时候使用 `docker rm -v` 这个命令。
    >
    > `docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py`  可以加载主机的 `src/webapp` 目录到容器中的 `/opt/webapp` 目录，这样即可挂载一个主机目录作为数据卷。dockerfile 中不支持这种用法，因为 dockerfile 是为了抑制和分享用的，然后不同的操作系统路径格式不一样，所以目前还不支持。
    >
    > 在主机中可以使用 `docker inspect` 来查看容器详细信息。
    >
    > 在 osx 中，查看数据卷的方法如下：
    >
    > ```python
    > screen ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/tty
    > cd /var/lib/docker
    > ```
    >
    > ​
    >
    > ***数据卷容器***： 如果有一些持续更新的数据需要在不同容器之间共享，可以创建数据卷容器，其实就是一个正常的容器，专门用来提供数据卷给其他容器挂载。两步走：
    >
    > - `docker run -d -v /dbdata --name dbdata training/postgres` 创建一个数据卷容器
    > - `docker run -d --volumes-from dbdata --name db1 training/postgres` 在其他容器中使用 `--volumes-from` 来挂载 dbdata 容器中的数据卷。

  - 使用网络

    > ***外部访问容器***： 容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过 `-P, -p` 来制定端口映射。当使用 `-P` 时，docker 会随机映射一个 49000～49900 的端口到内部容器开放的网络端口。使用 `-p` 时可以制定要映射的端口，并且在一个指定端口上可以只绑定一个容器，支持的格式有：`ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort`。后面可以使用 `docker port` 来查看当前映射的端口配置。
    >
    > ***容器互联***： 容器的连接 linking 系统是除了端口映射外，另一种跟容器中应用交互的方式，该系统会在源和接收容器之间创建一个隧道，接收容器可以看到源容器指定的信息。

  - 高级网络配置

    > 略

  - 安全

    > 略

  - 底层实现

    > docker 底层的核心包括 linux 上的命名空间 namespaces，控制组 control groups，union 文件系统 和 容器格式 container format。
    >
    > ***基本架构***： docker 采用了 c/s 架构，docker daemon 作为服务端接受来自客户端的请求，并处理这些请求。客户端和服务端既可以运行在一个机器上，也可以通过 socket 或者 restful api 来进行通信。
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/2cb2257a-b56f-11e7-95fa-0242ac140002)
    >
    > ***命名空间***： 每个容器都有自己的命名空间，运行在其中的应用都像是在独立的操作系统中一样，命名空间保证了容器之间彼此互不影响。比如说有 pid 命名空间，net 命名空间，ipc 命名空间，mnt，uts，user 命名空间。
    >
    > ***控制组***： cgroups 是linux内核的一个特性，主要用来对共享资源进行隔离，限制，审计等，只有能控制分配到容器的资源，才能避免当多个容器同时运行时对系统资源的竞争。它可以对容器的内存，cpu，磁盘io等资源进行限制和审计。
    >
    > ***联合文件系统***： unionfs是一种分层，轻量级并且高性能的文件系统，他支持对文件系统的修改按照一层一层来叠加。
    >
    > ***容器格式***：最初采用 lxc 中的容器格式，后来发展为 libcontainer，现在应该是演化为 runc 和 containerd。
    >
    > ***网络***：docker 的网络实现骑士就是利用了 linux 上的网络命名空间和虚拟网络设备（特别是 veth pair）。


- answering
  - 无问题阅读，纯学习
- questions
  - 无
- reviews
  - 无



## 5. 2017.10 | Tensorflow - 实战 google 深度学习框架

- reason

  - alphaGo 第一代被打败的消息着实震惊了我，虽然还不知道深度学习在其他工业化方面有什么具体的应用，但是我觉得这确实是在暗示一个新的时代的到来，所以我觉得非常有必要学习，掌握一些深度学习的能力。

- what

  - 第一章：深度学习简介

    > - 人工智能，机器学习和深度学习：总的来说，人工智能，机器学习和深度学习是非常相关的几个领域。人工智能是一类非常广泛的问题，机器学习是姐姐这类问题的一个重要手段，深度学习则是机器学习的一个分支。在很多人工智能问题上，深度学习突破了传统机器学习的瓶颈，推动了人工智能领域的发展。
    >
    > - 发展历程：早期的神经网络模型类似于仿生机器学习，它试图模仿人类大脑的学习机理。它们都有一些输入，然后将输入进行一些变换后得到输出结果，将这些输出结果加权得到一个阈值函数后输出0或者1。为了使计算机能够合理的设置那些权重，frank rosenblatt 教授在 1958年提出了感知机模型，感知机是首个可以根据数据样例来学习特征权重的模型。但它有一个非常大的局限，即只能解决线性可分问题，无法解决异或问题。
    >
    >   第二波神经网络研究因为分布式知识表达和神经网络反向传播算法的提出而兴起。比如说要设计一个模型来识别不同颜色不同型号的汽车，第一种方法是设计一个模型使得每一个神经元对应一种颜色和汽车型号的组合，如果有n种颜色，m种型号，那么一共需要n*m个神经元；另一种方法是使用一些神经元专门表示颜色，另外一些神经元专门表示汽车型号，这种方式只需要n + m 个神经元。分布式知识表达大大加强了模型的表达能力，使神经网络从宽度的方向走向了深度的方向，为之后的深度学习奠定了基础。而反向传播算法大幅降低了网络的训练时间。同时该阶段传统的机器学习也有了较大发展，并且由于计算机速度，数据量的限制，深度学习并没有体现出比较好的性能。
    >
    >   第三波是伴随计算机性能的提高，大数据的兴起，解决了神经网络历史以来的几个难题，使其迎来新的高潮期。
    >
    > - 应用：图像识别，语音识别，音频处理，自然语言处理，机器人，电脑游戏，搜索引擎，广告投放等；

  - 第二章：tensorflow 环境搭建

    > - 主要依赖：protocol buffer 和 bazel。其中 protocol buffer 是google开发的，类似于 xml 和json 的用于处理结构化数据的工具。而 ***处理结构化*** 数据一般即指将规范化格式的数据转变成可以直接在网络中进行传输的数据流，并且在接收端可以将数据流再转变成原始格式的数据，即所谓的序列化和反序列化。protocol buffer序列化之后的数据是二进制流，xml和json都是可读的字符串；其次 protocol buffer 需要数据格式定义文件，而 xml 和 json 不需要；因此 protocol buffer 序列出来的数据要比 xml 小 3～10倍，解析时间要快 20～100倍。bazel 是 google 开源的自动化构建工具。
    >
    > - 按照后测试
    >
    >   ```python
    >   import tensorflow as tf
    >   a = tf.constant([1, 2], name='a')
    >   b = tf.constant([3, 4], name='b')
    >   result = a + b
    >   sees = tf.Session()
    >   print sess.run(result)
    >   ```

  - 第三章：tensorflow 入门

    > - tensorflow 计算模型-计算图：计算图是tf中的最基本的概念，tf中所有计算都会被转化为计算图上的节点。其中 tensor 代表张量，可以简单的理解为多维数组，flow 代表计算流，体现了 tensor 的计算流程。所以tensorflow的程序一般分为两个阶段：***第一个阶段需要定义计算图中所有的计算；第二阶段执行这些计算***。tf 会自动将用户定义的计算转化为计算图上的节点，系统会默认维护一个默认的计算图，可以通过 `tf.get_default_graph` 来查看当前使用的计算图。也支持 `tf.Graph` 来生产新的计算图，不同计算图上的张量和运算流都不会共享。tf 中的计算图不仅仅可以用来隔离张量和计算，还可以提供管理张量和计算的机制，比如说计算图可以通过 tf.Graph.device 函数来指定计算的设备，比如说是选择 cpu 还是 gpu。
    >
    >   ```python
    >   g1 = tf.Graph()
    >   with g1.as_default():
    >     v = tf.get_variable('v', initializer=tf.zeros_initializer(shape=[1]))
    >     
    >   g2 = tf.Graph()
    >   with g2.as_default():
    >     v = tf.get_variable('v', initializer=tf.zeros_initializer(shape=[1]))
    >     
    >   with tf.Session(graph=g1) as sess:
    >     ...
    >
    >   with g1.device('/gpu:0'):
    >     ...
    >   ```
    >
    > - tf 的数据模型 - 张量
    >
    >   > 张量可以简单的理解为多维数组，其中零阶张量表示标量（scalar），也就是一个数；第一阶张量为向量（vector），即一个一维数组；第n阶张量可以理解为一个n维数组。但张量在 tf 中的实现并不是直接采用数组的形式，它只是对 tf 中运算结果的引用。在张量中并没有真正保存数字，而是保存如何得到这些数字的计算过程。一个张量中主要保存了三个属性：名字，维度，类型。
    >   >
    >   > 张量的使用一般有两个目的：第一是对中间计算结果的引用；第二是计算图构造完成后，可以用张量来获取计算结果。
    >
    > - tf的运行模型 - 会话
    >
    >   > 会话（session）用来执行定义好的运算，它拥有并管理 tf 程序运行时的所有资源，当计算完成后需要关闭会话来帮助系统回收资源，否则可能出现资源泄漏的问题。tf 中有两种模式来写作程序：
    >   >
    >   > ```python
    >   > #### method 1
    >   > ## create a session
    >   > sess = tf.Session()
    >   > ## run a program
    >   > sess.run(...)
    >   > ## close a session, but will skip it when exception occurs before
    >   > sess.close()
    >   >
    >   > #### method 2: better way
    >   > with tf.Session() as sess:
    >   >   sess.run(...)
    >   > ```
    >   >
    >   > 在交互环境下可以使用交互式会话 `tf.InteractiveSession`。使用 `tf.InteractiveSession` 可以省去将产生的会话注册为默认会话的过程。无论通过什么方式，都可以通过配置来生成新的会话，比如说：
    >   >
    >   > ```python
    >   > config = tf.ConfigProto(
    >   >           allow_soft_placement = True, 
    >   >             log_device_placement = True,
    >   > )
    >   > sess = tf.Session(config=config)
    >   > ```
    >   >
    >   > 配置中当 allow_soft_placement 为 true 时，可以将 gpu 的运算放到 cpu 上进行，只要满足下面几个条件即可：运算无法在 gpu 上进行；没有 gpu 资源；运算输入包含对 cpu 结果的引用。
    >
    >
    > - tf 实现神经网络
    >
    >   > 使用神经网络解决分类问题主要可以分为下面几个步骤：
    >   >
    >   > 1. 提取问题中实体店特征向量作为神经网络的输入；
    >   > 2. 定义神经网络的结构，并定义如何从神经网络的输入得到输出，即定义神经网络的前向传播算法；
    >   > 3. 训练模型来获得神经网络中的模型参数；
    >   > 4. 使用训练好的神经网络来预测数据；
    >   >
    >   > ***前向传播算法***：一个神经元有多个输入和一个输出，每个神经元的输入也可以是其他神经元的输出。所谓神经网络结构就是指不同的神经元之间的连接结构。如下图，不同的输入权重就是神经元的参数，神经网络的优化就是优化这些参数。
    >   >
    >   > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/53ecb42e-b7b9-11e7-95fa-0242ac140002)
    >   >
    >   > 下面是一个判断零件是否合格的三层全联接神经网络，所谓全联接是指相邻两层之间的任意两个节点都是有连接的，这同之后要介绍的卷积层，lstm 结构区分开来。
    >   >
    >   > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/b7345a14-b7b9-11e7-95fa-0242ac140002)
    >   >
    >   > 前向传播算法可以表示为矩阵乘法，如下：
    >   >
    >   > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/552d11ca-b7ba-11e7-9497-0242ac140002)
    >   >
    >   > 神经网络中的参数的组织，保存，更新都是放在 tf.Variable 中的，通常神经网络的参数都是初始化为随机值，下面一段代码就是在 tf 中声明并初始化一个 2x3 矩阵变量的方法，参数均值为0，标准差为2：
    >   >
    >   > ```python
    >   > weights = tf.Variable(tf.random_normal([2, 3], stddev=2))
    >   > ```
    >   >
    >   > 在 tf 中，一个变量的值在被使用之前，这个变量的初始化过程需要明确的被调用。tf 中的核心概念是张量，所有的数据都是通过张量来组织的，刚才介绍的 变量 是通过函数 tf.Variable 的一个运算，运算结果就是一个张量，所以变量就是一种特殊的张量而已。
    >   >
    >   > 在神经网络优化算法中，最常用的是反向传播算法（backpropagation）：
    >   >
    >   > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/be6d70a6-b7bc-11e7-9497-0242ac140002)
    >   >
    >   > 图中在得到一个 batch 的前向传播结果后，需要定义一个损失函数来刻画当前的预测值和真实值之间的差距，然后通过反向传播算法来调整神经网络参数的取值使得差距可以被缩小。
    >   >
    >   > 下面是一个完整的神经网络解决二分类问题：
    >   >
    >   > ```python
    >   > import tensorflow as tf
    >   > from numpy.random import RandomState
    >   >
    >   > # 定义训练数据的 batch 大小
    >   > batch_size = 8
    >   >
    >   > # 定义神经网络的参数，同时也是在定义神经网络的结构
    >   > w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))
    >   > w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))
    >   >
    >   > # 
    >   > x = tf.placeholder(tf.float32, shape=(None, 2), name='x-input')
    >   > y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y-input')
    >   >
    >   > # 定义神经网络前向传播的过程
    >   > a = tf.matmul(x, w1)
    >   > y = tf.matmul(a, w2)
    >   >
    >   > # 定义损失函数和反向传播算法
    >   > cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))
    >   > train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)
    >   >
    >   > # 随机生成一个模拟数据集
    >   > rdm = RandomState(1)
    >   > dataset_size = 128
    >   > X = rdm.rand(dataset_size, 2)
    >   >
    >   > # 
    >   > Y = [[int(x1 + x2 < 1)] for (x1, x2) in X]
    >   >
    >   > # 
    >   > with tf.Session() as sess:
    >   >     init_op = tf.global_variables_initializer()
    >   >     sess.run(init_op)
    >   >     print sess.run(w1)
    >   >     print sess.run(w2)
    >   >
    >   >     steps = 5000
    >   >     for i in range(steps):
    >   >         start = (i * batch_size) % dataset_size
    >   >         end = min(start + batch_size, dataset_size)
    >   >         # 通过选取的样本训练神经网络并且更新参数
    >   >         sess.run(train_step, 
    >   >                  feed_dict={x: X[start: end], y_: Y[start: end]})
    >   >         if i % 1000 == 0:
    >   >             total_cross_entropy = sess.run(cross_entropy, feed_dict={x: X, y_: Y})
    >   >             print total_cross_entropy
    >   > ```

  - 第四章：深层神经网络

    > ***深度学习与深层神经网络***： 一般来说，可以简单的把深度学习理解为深层神经网络，维基百科对深度学习的定义是：*一类通过多层非线性变换对高复杂性数据建模算法对合集*，其中关键特性就是 ***多层*** 和 ***非线性***。
    >
    > 线性模型最大的特点就是任何线性模型的组合仍然还是线性模型。有时候简单的前向传播算法也可以用线性模型来表示。然而线性模型能解决的问题都是有限的，这也是线性模型的局限性。比如下面这种问题，（激活函数采取线性函数）：
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/935b737a-b7c3-11e7-95fa-0242ac140002)
    >
    > 一般来说都是通过激活函数来改变模型的线性结构，即通过非线性激活函数来将线性模型改变成非线性模型，下面是一些常见的非线性函数：
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/02268e0c-b7c4-11e7-95fa-0242ac140002)
    >
    > 目前 tf 提供了 7 个不同的非线性激活函数，当然也可以自定义自己的激活函数。
    >
    > ***损失函数定义***：神经网络模型的效果以及优化的目标都是通过损失函数来定义和度量的。一般通过交叉熵（cross entropy）来评价一个输出向量和期望向量之间的接近程度，交叉熵是一个信息论中的概念，原本用于估算平均编码长度，对于两个概率分布 p 和 q，通过 p 和 q 来计算交叉熵为：
    >
    > ==$$H(p, q) = - \sum p(x)log(q(x))$$==
    >
    > 然而需要注意交叉熵刻画的是两个概率分布之间的距离，然后神经网络的输出却不一定是一个概率分布。但是任意事件发生的概率都在0和1之间，且总有某一个事件发生（概率和为1），如果将分类问题中 “一个样例属于某一个类别” 看成一个概率事件，那么训练数据的正确答案就符合一个概率分布。因为事件 “一个样例属于不正确的类别” 的概率为0，而 “一个样例属于正确的类别” 的概率为1，此时可以通过 softmax 回归将神经网络前向传播得到的结果变成概率分布。假使神经网络的输出为 y1, y2, … yn，那么经过 softmax 回归后的输出为：
    >
    > $$softmax(y)_i = y_i^{,} = \frac {e_{}^{yi}}{\sum_{j=1}^{n} e_{}^{yj}}$$
    >
    > 从上面公式可以看到，原始神经网络的输出被用作置信度来生成新的输出，而新的输出满足概率分布的所有要求。这个新的输出可以理解为经过神经网络的推导，一个样例为不同类别的概率分别是多少，这样就直接把神经网络的输出也变成一个概率分布了，从而可以通过交叉熵来计算预测的概率分布和真实答案概率分布之间的距离。
    >
    > 从交叉熵的公式可以看到交叉熵函数不是对称的 $H(p, q) \neq H(q, p) $，它刻画的是通过概率分布 p 来表达概率分布 q 的困哪程度。因为正确答案是希望得到正确的结果，所以当交叉熵作为神经网络的损失函数时，p代表的是正确答案，q代表的是预测值。交叉熵刻画的是两个概率分布的距离，也就是说交叉熵越小，两个概率分布越接近。
    >
    > 既然这里有点抽象，难以理解，那就看个例子呗：
    >
    > 假设有一个三分类问题，某个样例的的正确答案是（1，0，0），某模型经过 softmax 回归后的预测答案是（0.5，0.4，0.1），那么这个预测答案和正确答案之间的交叉熵为：
    >
    > - $$H((1,0,0), (0.5,0.4,0.1)) = -(1*log0.5 + 0*log0.4 + 0*log0.1) = 0.3$$
    >
    >   ​
    >
    > 如果另外一个模型的预测是（0.8，0.1，0.1），那么这个模型的预测值和真实值的交叉熵是：
    >
    > - $$H((1,0,0), (0.8, 0.1, 0.1)) = -(1*log0.8 + 0*log0.1 + 0*log0.1) = 0.1$$
    >
    > ​
    >
    > 从直观上说第二个模型会的答案优于第一个，通过交叉熵计算出来的也是同样的结论。tf 中计算交叉熵的代码也很简单 `cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))` ，其中 y_ 代表正确结果，y 代表预测结果。通过 tf.clip_by_value 函数可以将一个张量的数值限制在一个范围之内，这样可以避免一些错误，比如 log0 是无效的。
    >
    > 与分类问题不同，回归问题的预测是连续变量，对这类问题最常用的损失函数就是均方误差（mse：mean squared error），它的定义如下：
    >
    > $$MSE(y, y_{'}) = \frac {\sum_{i=1}^{n} (y_i - y_i^{'})_{}^{2}} {n}$$
    >
    > tf中实现均方误差的函数 `mse = tf.reduce_mean(tf.square(y_ - y))` 。
    >
    > ***神经网络优化算法***： 常用的优化算法有反向传播算法和梯度下降算法，梯度下降算法主要用于优化单个参数，而反向传播给出了一个高效的方式在所有参数上使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小。
    >
    > ***神经网络的进一步优化***： 学习率（learning rate）的设置，太小的话会导致训练次数太多，降低优化速度，太大的话可能优化结果不好，所以一般可以采用可变学习率的方法，比如说指数衰减法，这种情况下一开始时学习率很大，最后学习率会越来越小。
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/7164f204-be1d-11e7-9497-0242ac140002)
    >
    > 过拟合问题也经常出现在机器学习，深度学习问题中，比如说，举一个极端的例子，如果一个模型中的参数比训练数据的总数还要多，那么只要训练数据不冲突，这个模型完全可以记住所有训练数据的结果从而使得损失函数为0.为了避免过拟合，一个非常常用的方法是正则化，其思想是在损失函数中加入刻画模型复杂度的指标。比如：假设损失函数为 $J(\theta)$，那么在优化时可以优化函数 $J(\theta) + \lambda R(w)$，其中 $R(w)$ 刻画的是模型的复杂程度，而 $\lambda$ 表示模型复杂损失在总损失中占的比例。常用的刻画模型复杂度的函数 $R(w)$ 有两种，一种是L1正则化：
    >
    > $$R(w) = \parallel w \parallel_1 = \sum \mid w_i \mid$$
    >
    > 另一种是 L2 正则化，计算公式是：
    >
    > $$R(w) = \parallel w \parallel_{2}^{2} = \sum \mid w_i^{2} \mid$$
    >
    > 无论哪一种正则化，其原理都是希望通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪声。但这两种正则化会有很大差别，具体看书或者百科，所以实践中一般可以将两种方法同时使用：
    >
    > $$R(w) = \sum_i \alpha \mid w_i \mid + (1 - \alpha) w_i^2$$

  - 第五章：MNIST 数字识别问题

    > ***MNIST数据处理***
    >
    > ***神经网络模型训练及不同模型结果对比***：例子代码不错，比较完整；
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/ebe7c362-be22-11e7-811a-0242ac140002)
    >
    > ***变量管理***：
    >
    > ***TensorFlow模型的持久化***：
    >
    > ***TensorFlow最佳实践样例程序***：代码例子不错，实现自己体会一下。

  - 第六章：图像识别与卷积神经网络

    > ***图像识别问题简介及经典数据集***：
    >
    > ***卷积神经网络介绍***： 卷积神经网络相邻两层之间只有部分节点相连，为了展示每一层神经元的维度，一般会将每一层卷积层的节点组织成一个三维矩阵；除了结构相似，卷积神经网络的输入输出及训练流程与全联接神经网络也基本一致；
    >
    > 卷积神经网络的引入：使用全联接神经网络处理图像的最大问题就是全联接层的参数太多。比如对于 mnist 数据集，每一个图片大小是 28x28x1，假设第一层隐藏层的节点数为500个，那么一个全联接层的神经网络就有 28x28x500 + 500 = 392500 个参数，当图片更大时，比如 cifar-10 数据集，图片大小 32x32x3，这样输入层就有 3072 个节点，如果第一层隐藏层仍然有500个节点，则这一层全联接神经网络就有 3072x500 + 500=150m 个参数。参数过多除了导致计算速度减慢，还很容易导致过拟合问题。而卷积神经网络就是一种***有效的减少神经网络中参数个数的算法***。
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/25131b7c-c040-11e7-811a-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/7988227c-c042-11e7-abaa-0242ac140002)
    >
    > 一般卷积神经网络结构：输入层 -> 卷积层 -> 池化层 -> 全联接层 -> softmax 层
    >
    > ***卷积神经网络常用结构***： 
    >
    > 卷积层：也被称为过滤器或内核，它将当前神经网络层的一个子节点矩阵转化为下一层神经网络的一个单位节点矩阵。![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/88f61294-c05d-11e7-abaa-0242ac140002)
    >
    > 关于 tf 中如何定义卷积层，可以看原书，非常详细。
    >
    > 池化层：池化层可以有效的缩小矩阵的尺寸，从而减少最后全联接层中的参数，使用池化层即可以加快计算速度也有防止过拟合的作用。池化层的前向传播也类似卷积层一样需要一个过滤器，不过不是计算节点的加权和，而是简单的最大值或者平均值。使用最大值操作的池化层叫最大池化层。
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/0ebfc7fc-c05f-11e7-abaa-0242ac140002)
    >
    > ***经典卷积网络模型***：
    >
    > LeNet-5: 第一个被成功应用于数字识别问题上的卷积神经网络，在 mnist 数据集上大约可以做到 99.2 的正确率。
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/6356ad3e-c060-11e7-abaa-0242ac140002)
    >
    > 基于 tf 实现的代码可以在原书看到，似乎 tf 来写神经网络真的挺方便的嘛。
    >
    > Inception-v3 模型：LeNet-5 是将不同的卷积层通过串联的方式连接在一起，而 Inception-v3 中是将不同的卷积层通过并联的方式结合在一起。
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/97a41abc-c061-11e7-abaa-0242ac140002)
    >
    > 下面是 Inception-v3 模型架构图，Inception-v3 有 96 个卷积层，原始的代码写法会比较复杂。tf 提供了 slim 这个工具，可以非常简单的实现一个卷积层，完整的代码直接参考原书即可，果然，tf 和 spark 一样，都是有很强的实践指导：
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/fc436c46-c063-11e7-811a-0242ac140002)
    >
    > ***卷积神经网络迁移学习***：所谓迁移学习，就是指将一个问题上训练好的模型通过简单的调整使其适用于一个新的问题。

  - 第七章：图像数据处理

    > ​

- answering

- question

- reviews

  - 相关公司：bitfusion，caicloud，
  - [tensorflow学习笔记（三十八）:损失函数加上正则项](http://blog.csdn.net/u012436149/article/details/70264257)
  - [TensorFlow四种Cross Entropy算法实现和应用](https://weibo.com/ttarticle/p/show?id=2309404047468714166594&ssl_rnd=1509591642.4778)
  - [tf.cond 与 tf.control_dependencies 的控制问题](http://yanjoy.win/2017/04/18/tfcond/)


## 6. 2017.12 | 思考，快与慢

- ***reason***: 

  > 考完 cfa 了，11月也没有看其他书，这个月休息了几天，就要把读书计划继续起来了，这本书也是在今年11,12月计划里的。第一次听说这本书是2年前宇迪推荐的，然后一直在书单里，加上今年诺贝尔经济学颁给了行为经济学家，我想也可以把这本书详细读读呗，算是对思考，心理，经济方面的一些了解咯。

- ***what***: 

  > 作者：丹尼尔 卡尼曼，美国心理学家，获2002年诺贝尔经济学奖。
  >
  > 提出大脑中两套思维系统，系统1的运行是无意识且迅速的，不用进行思考就能做出决策，是完全自主空值的；系统2将注意力转移到需要耗费脑力的活动上来，如复杂的运算等。
  >
  > 我们会忽略一些显而易见的事，也会忽视自己屏蔽了这些事的事实。
  >
  > 缪勒莱耶错觉图便是系统1和系统2对比的一个好例子。
  >
  > 一个人对某个东西比较感兴趣或正在进行深度思考时瞳孔会变大。
  >
  > 在经济行为中，付出就是成本，学习技能就是为了追求利益和成本的平衡，因为懒惰是人类的本性。
  >
  > 聪明的人在大多数事情上的表述都要比其他人表述得更好，聪明不仅仅指推理的能力，还有在记忆中搜寻相关信息和在必要时调动注意力的能力。
  >
  > 想让人们相信谬误有个可靠的方法，那就是不断的重复，因为人们很难对熟悉感知的事物和真相加以区别。
  >
  > 《怎样解题》中提到，“如果你无法解决一个问题，就去解决另一个简单一点的问题好了------去找这个简单的问题吧。”，波利亚的启发法是系统2的有意实施战略性决策的过程。
  >
  > 锚定效应，人们对某一未知量就行评估之前，总会事先对这个量就行一番考量；一旦医药考虑某个数字是否会成为一个估测问题的可能答案时，这个数字就会产生锚定效应。人们的判断明显会收到一些没有任何价值的数字的影响。
  >
  > ​

- ***Answering***: Does the book answer your questions

- ***questions:*** Any questions after reading

- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews




## 7. 2017.12 | 零基础入门深度学习

七篇高质量文章，当一本书来研读，地址：https://www.zybuluo.com/hanbingtao/note/433855

- ***reason***: 既然看好和决定在深读学习方面有一些了解和经验，那就值得专心学习一下，get busy learning or get busy dying.

- ***what***: 

  > 这本书的内容我觉得完全可以跟 http://neuralnetworksanddeeplearning.com/ 的内容媲美了，而且这系列文章都写得很详细，代码公式也超全，专心看下来收货肯定很高。

  - 感知器

    > 人工智能 -> 机器学习 -> 深度学习；
    >
    > 神经网络结构：输入层 -> 隐藏层 -> 输出层；
    >
    > 隐藏层比较多（大于2）的神经网络叫做深度神经网络。而深度学习，就是使用深层架构（比如，深度神经网络）的机器学习方法。
    >
    > 感知器，即神经元，由三部分构成：输入权重，激活函数，输出；感知器的输入由下面这个公式来
    >
    > $y=f(\mathrm{w}\bullet\mathrm{x}+b)\qquad 公式(1)$, 
    >
    > 感知器不仅仅能实现简单的布尔运算。它可以拟合任何的线性函数，任何**线性分类**或**线性回归**问题都可以用感知器来解决。
    >
    > 然而，感知器却不能实现异或运算，如下图所示，异或运算不是线性的，你无法用一条直线把分类0和分类1分开。
    >
    > ![](http://upload-images.jianshu.io/upload_images/2256672-9b651d237936781c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/360)
    >
    > 感知器训练算法：将权重项和偏置项初始化为0，然后，利用下面的**感知器规则**迭代的修改 $w_i$和 $b$，直到训练完成。
    >
    > $\begin{align}w_i&\gets w_i+\Delta w_i \\b&\gets b+\Delta b\end{align}$
    >
    > $\begin{align}\Delta w_i&=\eta(t-y)x_i \\\Delta b&=\eta(t-y)\end{align}$
    >
    > $w_i$ 是输入 $x_i$ 的权重值，b 是偏置项，事实上可以把 b 看作值永远为1 的输入 $x_b$ 的权重。 t 是训练样本的实际值，一般称为 label，而 y 是感知器的输出值，$\eta$  是学习速率，可以是常数，也可以是一个收敛的变量，作用是控制每一步调整权重的幅度。
    >
    > 感知器训练的过程为：每次从训练数据中取出一个样本的输入向量，使用感知器计算其输出，再根据上面的规则来调整权重。每处理一个样本就调整一次权重。经过多轮迭代后（即全部的训练数据被反复处理多轮），就可以训练出感知器的权重，使之实现目标函数。

  - 线性单元和梯度下降

    > 感知器有一个问题，当面对的数据集不是**线性可分**的时候，『感知器规则』可能无法收敛，这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个**可导**的**线性函数**来替代感知器的**阶跃函数**，这种感知器就叫做**线性单元**。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。
    >
    > 为了简单起见，我们可以设置线性单元的激活函数为 $f(x) = x$, 这样的线性单元如下图所示:
    >
    > ![](http://upload-images.jianshu.io/upload_images/2256672-f57602e423d739ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
    >
    > 这样替换了激活函数之后，**线性单元**将返回一个**实数值**而不是**0,1分类**。因此线性单元用来解决**回归**问题而不是**分类**问题。
    >
    > 模型为： $y=h(x)=\mathrm{w}^T\mathrm{x}\qquad\qquad(式1)$， 称作线性模型，其中输出 y 就是输入特征 $x_1, x_2, ... x_n$ 的线性组合，对于这样一个模型来说，目标函数是最小化训练集的误差，即
    >
    > $\begin{align}E(\mathrm{w})&=\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\\&=\frac{1}{2}\sum_{i=1}^{n}(\mathrm{y^{(i)}-\mathrm{w}^Tx^{(i)}})^2\end{align}$
    >
    > 这类问题在数学上称作优化问题，而 $E(w)$ 就是我们的优化的目标，也叫目标函数。
    >
    > 函数 $y=f(x)$ 的极值点，就是它的导数为0 的那些点，因此我们也可以通过解方程 $f'(x)=0$ 来求函数 $f(x)$  的极值。不过计算机不会解方程也不会求导，只能通过试错来接近极值点。
    >
    > ![](http://upload-images.jianshu.io/upload_images/2256672-46acc2c2d52fc366.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480)
    >
    > 所谓梯度，是指一个向量，并且该向量指向函数值上升最快的方向，反之，梯度的反方向就是函数值下降最快的方向了。梯度下降算法的公式：$\mathrm{x}_{new}=\mathrm{x}_{old}-\eta\nabla{f(x)}$， 其中 $\nabla$ 是梯度算子。由于$\begin{align}E(\mathrm{w})&=\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\\&=\frac{1}{2}\sum_{i=1}^{n}(\mathrm{y^{(i)}-\mathrm{w}^Tx^{(i)}})^2\end{align}$，
    >
    > 所以这个优化函数里的梯度算法应用是：$\mathrm{w}_{new}=\mathrm{w}_{old}-\eta\nabla{E(\mathrm{w})}$, 下面我们只需要研究怎么计算 $\nabla E(w)$ 即可，关于它的推导可以直接看下面，结论先放上来，$\nabla{E(\mathrm{w})}=-\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}^{(i)}$, $\mathrm{w}_{new}=\mathrm{w}_{old}+\eta\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}^{(i)}\qquad\qquad(式3)$， 或者：
    >
    > $\begin{bmatrix}w_0 \\w_1 \\w_2 \\... \\w_m \\\end{bmatrix}_{new}=\begin{bmatrix}w_0 \\w_1 \\w_2 \\... \\w_m \\\end{bmatrix}_{old}+\eta\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\begin{bmatrix}1 \\x_1^{(i)} \\x_2^{(i)} \\... \\x_m^{(i)} \\\end{bmatrix}$,
    >
    > - ***推导：$\nabla E(w)$*** 
    >
    > ----
    >
    > -  函数的梯度的定义就是它相对于各个变量的**偏导数**
    >
    > $\begin{align}\nabla{E(\mathrm{w})}&=\frac{\partial}{\partial\mathrm{w}}E(\mathrm{w})\\&=\frac{\partial}{\partial\mathrm{w}}\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\\\end{align}$,
    >
    > - 和的导数等于导数的和
    >
    > $\begin{align} &\frac{\partial}{\partial\mathrm{w}}\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\\ =&\frac{1}{2}\sum_{i=1}^{n}\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\\end{align}$,
    >
    > 现在我们可以不管高大上的 $\sum$ 了，先专心把里面的导数求出来:
    >
    > $\begin{align} &\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\ =&\frac{\partial}{\partial\mathrm{w}}(y^{(i)2}-2\bar{y}^{(i)}y^{(i)}+\bar{y}^{(i)2})\\\end{align}$,
    >
    > 链式求导法:
    >
    > $\frac{\partial{E(\mathrm{w})}}{\partial\mathrm{w}}=\frac{\partial{E({w})}}{\partial\bar{y}}\frac{\partial{\bar{y}}}{\partial\mathrm{w}}$,
    >
    > 利用上面的链式求导，分别计算式子右边的两个乘数：
    >
    > $\begin{align} \frac{\partial{E(\mathrm{w})}}{\partial\bar{y}}= &\frac{\partial}{\partial\bar{y}}(y^{(i)2}-2\bar{y}^{(i)}y^{(i)}+\bar{y}^{(i)2})\\ =&-2y^{(i)}+2\bar{y}^{(i)}\\\\ \frac{\partial{\bar{y}}}{\partial\mathrm{w}}= &\frac{\partial}{\partial\mathrm{w}}\mathrm{w}^T\mathrm{x}\\ =&\mathrm{x}\end{align}$,
    >
    > 代入，我们求得 $\sum$ 里面的偏导数是:
    >
    > $\begin{align} &\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\ =&2(-y^{(i)}+\bar{y}^{(i)})\mathrm{x}\end{align}$ ,
    >
    > 最后代入 $\nabla E(w)$，求得:
    >
    > $\begin{align} \nabla{E(\mathrm{w})}&=\frac{1}{2}\sum_{i=1}^{n}\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\ &=\frac{1}{2}\sum_{i=1}^{n}2(-y^{(i)}+\bar{y}^{(i)})\mathrm{x}\\ &=-\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x} \end{align}$,
    >
    > 至此，推算完成，其实就是用了求导的一些法则。
    >
    > ----
    >
    > 如果我们根据(式3)来训练模型，那么我们每次更新的迭代，要遍历训练数据中所有的样本进行计算，我们称这种算法叫做 **批梯度下降(Batch Gradient Descent)**。如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是SGD算法。在SGD算法中，每次更新 w 的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对 w 更新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新 w 并不一定按照减少 E 的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少 E 的方向前进的，因此最后也能收敛到最小值附近。下图展示了SGD和BGD的区别:
    >
    > ![](http://upload-images.jianshu.io/upload_images/2256672-3152002d503d768e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240),
    >
    > - 感知器和线性单元区别，激活函数不同而已；
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/2c120af8-df28-11e7-ba0d-0242ac140002)
    >
    > ​

  - 神经网络和反向传播算法

    > 神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数，而当我们说神经元的时候，激活函数往往是 sigmoid 函数或者 tanh 函数：
    >
    > ![](http://upload-images.jianshu.io/upload_images/2256672-49f06e2e9d3eb29f.gif)
    >
    > $y = f(x) = sigmoid(x)=\frac{1}{1+e^{-x}}$, 其导数很特殊 $y' = y(1 - y)$，
    >
    > 神经网络实际上就是一个输入向量 $\vec x$ 到输出向量 $\vec y$ 的函数，即 $\vec y = f_{network} (\vec x)$， 
    >
    > 神经网络的数学表示，即上面的方程展开表示，以下面这个神经网络为例：
    >
    > ![](http://upload-images.jianshu.io/upload_images/2256672-bfbb364740f898d1.png)
    >
    > 首先是隐藏层四个节点的数学表达：
    >
    > $a_4=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})\\ a_5=sigmoid(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})\\ a_6=sigmoid(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})\\a_7=sigmoid(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})\\$,
    >
    > 接下来定义网络的输入向量 $\vec x$ 和隐藏层每个节点的权重向量 $\vec w_j$，设置：
    > $$
    > \begin{align}
    > \vec{x}&=\begin{bmatrix}x_1\\x_2\\x_3\\1\end{bmatrix}\\
    > \vec{w}_4&=[w_{41},w_{42},w_{43},w_{4b}]\\
    > \vec{w}_5&=[w_{51},w_{52},w_{53},w_{5b}]\\
    > \vec{w}_6&=[w_{61},w_{62},w_{63},w_{6b}]\\
    > \vec{w}_7&=[w_{71},w_{72},w_{73},w_{7b}]\\
    > f&=sigmoid
    > \end{align}
    > $$
    > 代入前面的式子可以得到：
    > $$
    > \begin{align}
    > a_4&=f(\vec{w_4}\centerdot\vec{x})\\
    > a_5&=f(\vec{w_5}\centerdot\vec{x})\\
    > a_6&=f(\vec{w_6}\centerdot\vec{x})\\
    > a_7&=f(\vec{w_7}\centerdot\vec{x})
    > \end{align}
    > $$
    > 其中 $\vec w$ 是 1 x 4 向量，$\vec x$ 是 4 x 1 向量，$\vec w * \vec x$ 是一个标量，即为隐藏层一个神经元的数值；现在我们把上述的四个 a 写到一个矩阵里，每个式子作为矩阵的一行，就可以完全用矩阵来表示了：
    > $$
    > \vec{a}=
    > \begin{bmatrix}
    > a_4 \\
    > a_5 \\
    > a_6 \\
    > a_7 \\
    > \end{bmatrix},\qquad W=
    > \begin{bmatrix}
    > \vec{w}_4 \\
    > \vec{w}_5 \\
    > \vec{w}_6 \\
    > \vec{w}_7 \\
    > \end{bmatrix}=
    > \begin{bmatrix}
    > w_{41},w_{42},w_{43},w_{4b} \\
    > w_{51},w_{52},w_{53},w_{5b} \\
    > w_{61},w_{62},w_{63},w_{6b} \\
    > w_{71},w_{72},w_{73},w_{7b} \\
    > \end{bmatrix}
    > ,\qquad f(
    > \begin{bmatrix}
    > x_1\\
    > x_2\\
    > x_3\\
    > .\\
    > .\\
    > .\\
    > \end{bmatrix})=
    > \begin{bmatrix}
    > f(x_1)\\
    > f(x_2)\\
    > f(x_3)\\
    > .\\
    > .\\
    > .\\
    > \end{bmatrix}
    > $$
    > 代入前面的式子即得：$\vec a = f(W * \vec x)$，这个式子说明神经网络**每一层的作用实际上就是先将输入向量左乘一个数组就行线性变量，得到一个新的向量，然后再对这个向量逐元素应用一个激活函数**。
    >
    > 我们可以说神经网络是一个模型，那么神经网络上每个神经元之间的连接上的权重值，就是这个模型的参数，也就是模型需要训练和学习的东西；然后模型的一些参数是不需要学习的，比如连接方式，网络层数，每层节点数，这些人为事先设置的参数称为 ***超参数***。而其他参数的训练，一般我们用***反向传播算法，也叫 back propagation，简称 BP算法*** 来进行训练。
    >
    > 我们同样以之前的模型来理解，假设每个训练样本为 $(\vec x, \vec t)$, 其中 $\vec x$ 是训练样本的特征，$\vec t$ 是样本的目标值；
    >
    > ![](http://upload-images.jianshu.io/upload_images/2256672-6f27ced45cf5c0d8.png)
    >
    > 第二节有$\mathrm{w}_{new}=\mathrm{w}_{old}-\eta\nabla{E(\mathrm{w})}$ 和 $\nabla{E(\mathrm{w})}=-\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}^{(i)}$,对于输出节点 i，令 $\delta_i$ 为该节点的误差项，即为第二节中的 $\nabla{E(\mathrm{w})}$, 比如上面模型中节点8的误差项 $\delta_8$ 应该是：
    > $$
    > \delta_8 = y_1(1 - y_1)(t_1 - y_1)
    > $$
    > 其中 y1 是节点8 的输出值【模型预测值】，t1 是节点8的目标值【真实值】。对于隐藏层节点则有：
    > $$
    > \delta_i=a_i(1-a_i)\sum_{k\in{outputs}}w_{ki}\delta_k\qquad
    > $$
    > 其中 $a_i$ 是节点 $i$ 的输出值，$w_{ki}$ 是节点 $i$ 到它的下一层节点 $k$ 的链接的权重，$\delta_k$ 是节点 $i$ 的下一层节点 $k$ 的误差项，例如对于隐藏层节点 4 来说，计算方法如下：
    > $$
    > \delta_4 = a_4(1 - a_4) * (w_{84} \delta_8 + w_{94} \delta_9)
    > $$
    > 最后更新每个连接上的权值：
    > $$
    > w_{ji}  \leftarrow w_{ji} + \eta \delta_j x_{ji}
    > $$
    > 其中 $w_{ji}$ 是节点 $i$ 到节点 $j$ 的权重，$\eta$ 是一个称为 ***学习速率*** 的常数，$\delta_j$ 是节点j 的误差项，$x_{ji}$ 节点 $i$ 传递给节点 $j$ 的输入，例如权重 $w_{84}$ 的更新方法如下：
    > $$
    > w_{84} \leftarrow w_{84} + \eta \delta_8 a_4
    > $$
    > 偏置项的输入值永远是1，例如节点 4 的偏置项 $w_{4b}$ 是权值更新算法如下：
    > $$
    > w_{4b} \leftarrow w_{4b} +\eta \delta_4
    > $$
    > 上面介绍了神经网络每个节点误差项的计算和权重更新算法，显然可以看到，计算每一个节点的误差项，都需要先计算每个与其连接的下一层的节点的误差项，这就要求误差项的计算顺序必须从输出层开始的，然后反向一次计算每个隐藏层的误差项，知道与输入层相连的那个隐藏层，即第一层隐藏层。这就是 ***反向传播算法*** 的名字含义，当所有节点的误差项计算完毕后，可以根据下面的式子来更新权重了：
    > $$
    > w_{ji}  \leftarrow w_{ji} + \eta \delta_j x_{ji}
    > $$
    > ***反向传播算法的推导***： 反向传播算法其实就是链式求导法则的应用。然而，这个如此简单且显而易见的方法，却是在Roseblatt提出感知器算法将近30年之后才被发明和普及的。对此，Bengio这样回应道：***很多看似显而易见的想法只有在事后才变得显而易见。***
    >
    > 下面，我们从数学角度来推导反向传播算法：
    >
    > ---
    >
    > - 先确定神经网络的目标函数，然后用随机梯度下降优化算法来求目标函数最小值时候的参数值。我们暂且取网络所有输出层节点的误差平方和作为目标函数：
    >   $$
    >   E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2
    >   $$
    >   其中 $E_d$ 是样本 $d$ 的误差。然后随机梯度下降对目标函数进行优化的过程如下：
    >   $$
    >   w_{ji} \leftarrow w_{ji} - \eta \frac {\partial E_d}{\partial w_{ji}}
    >   $$
    >   随机梯度下降算法需要求出误差 $E_d$ 对每个权重 $w_{ji}$ 的偏导数，也就是梯度，那怎么求呢？继续观察下图：
    >
    >   ![](http://upload-images.jianshu.io/upload_images/2256672-6f27ced45cf5c0d8.png)
    >
    >   观察上图，我们发现权重 $w_{ji}$ 仅能通过影响节点 $j$ 的输入值影响网络的其他部分，设 $net_j$ 是节点 $j$ 的加权输入，即：
    >   $$
    >   net_j = \overrightarrow w_j * \overrightarrow x_j = \sum_i w_{ji} x_{ji}
    >   $$
    >   $E_d$ 是 $net_j$ 的函数，而 $net_j$ 是 $w_{ji}$ 的导数，根据链导法则，有：
    >   $$
    >   \begin{align}
    >   \frac{\partial{E_d}}{\partial{w_{ji}}}&=\frac{\partial{E_d}}{\partial{net_j}}\frac{\partial{net_j}}{\partial{w_{ji}}}\\
    >   &=\frac{\partial{E_d}}{\partial{net_j}}\frac{\partial{\sum_{i}{w_{ji}}x_{ji}}}{\partial{w_{ji}}}\\
    >   &=\frac{\partial{E_d}}{\partial{net_j}}x_{ji}
    >   \end{align}
    >   $$
    >   上式中，$x_{ji}$ 是节点 $i$ 传递给节点 $j$ 的输入值，也就是节点 $i$ 的输出值，对于 $\frac {\partial  E_d}{\partial net_j}$ 的推导，需要区分 ***输出层*** 和 ***隐藏层***  两种情况。
    >
    >   对于 ***输出层*** 来说，$net_j$ 仅能通过节点 $j$ 的输出值 $y_j$ 来影响网络的其他部分，即 $E_d$ 是 $y_i$ 的函数，而 $y_j$ 是 $net_j$ 的函数，其中 $y_j = sigmoid(net_j)$，所以我们可以再次使用链导法则：
    >   $$
    >   \begin{align}
    >   \frac{\partial{E_d}}{\partial{net_j}}&=\frac{\partial{E_d}}{\partial{y_j}}\frac{\partial{y_j}}{\partial{net_j}}\\
    >   \end{align}
    >   $$
    >   考虑第一项：
    >   $$
    >   \begin{align}
    >   \frac{\partial{E_d}}{\partial{y_j}}&=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2\\
    >   &=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2\\
    >   &=-(t_j-y_j)
    >   \end{align}
    >   $$
    >   考虑第二项：
    >   $$
    >   \begin{align}
    >   \frac{\partial{y_j}}{\partial{net_j}}&=\frac{\partial sigmoid(net_j)}{\partial{net_j}}\\
    >   &=y_j(1-y_j)\\
    >   \end{align}
    >   $$
    >   将第一二项代入式子，得到：
    >   $$
    >   \frac{\partial{E_d}}{\partial{net_j}}=-(t_j-y_j)y_j(1-y_j)
    >   $$
    >   如果令 $\delta_j = - \frac {\partial E_d}{\partial net_j}$，也就是一个节点的误差项 $\delta$ 是网络误差对这个节点输入的偏导数的相反数，代入上式得到：
    >   $$
    >   \delta_j = (t_j - y_j) y_j (1 - y_j)
    >   $$
    >   再将这个式子代入梯度下降公式，得到：
    >   $$
    >   \begin{align}
    >   w_{ji}&\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}\\
    >   &=w_{ji}+\eta(t_j-y_j)y_j(1-y_j)x_{ji}\\
    >   &=w_{ji}+\eta\delta_jx_{ji}
    >   \end{align}
    >   $$
    >   ***输出层*** 的推导就到此为止了，下面看看 ***隐藏层*** 的 $\frac {\partial E_d}{\partial net_j}$ 推导：
    >
    >   首先我们定义节点 $j$ 的所有直接下游节点的集合 $Downstream(j)$，例如对于节点 4 来说，他的直接下游节点是8，9；可以看到 $net_j$ 只能通过影响 $Downstream(j)$ 再影响 $E(d)$。设 $net_k$ 是节点 $j$ 的下游节点的输入，则 $E_d$ 是 $net_k$ 的函数，而 $net_k$ 是 $net_j$ 的函数，因为 $net_k$ 有多个，我们应用全导数公式可以做如下推导：
    >   $$
    >   \begin{align}
    >   \frac{\partial{E_d}}{\partial{net_j}}&=\sum_{k\in Downstream(j)}\frac{\partial{E_d}}{\partial{net_k}}\frac{\partial{net_k}}{\partial{net_j}}\\
    >   &=\sum_{k\in Downstream(j)}-\delta_k\frac{\partial{net_k}}{\partial{net_j}}\\
    >   &=\sum_{k\in Downstream(j)}-\delta_k\frac{\partial{net_k}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{net_j}}\\
    >   &=\sum_{k\in Downstream(j)}-\delta_kw_{kj}\frac{\partial{a_j}}{\partial{net_j}}\\
    >   &=\sum_{k\in Downstream(j)}-\delta_kw_{kj}a_j(1-a_j)\\
    >   &=-a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}
    >   \end{align}
    >   $$
    >   至此我们也完成了隐藏层的推导，需要注意的是我们的这个推导都是假设激活函数是 sigmoid 函数来做的。如果激活函数不同，误差计算方式不同，网络连接结构不同，优化算法不同，则具体的规则也会不一样，但无论怎样，训练规则的推导方式都是一样的，应用链式法则就行推导。
    >
    > ---
    >
    > ​
    >
    > ​
    >
    > adaf
    >
    > adsf
    >
    > ​

- ***Answering***: Does the book answer your questions

- ***questions:*** Any questions after reading

- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews




## 8. 2018.01 | Python 源码剖析

- ***reason***: 准备深入了解下 Python 的一些机制了，为了以后更加高效的使用 Python。

- ***what***: 就不按照章节总结内容了，按关键字来

  - 内存管理机制

    >4 层内存管理结构：
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/a330659a-f0f7-11e7-b54b-0242ac140002)
    >
    >c语言中使用宏可以避免一次函数调用的开销。python 2.5 中整个小块内存池可以看为一个层次结构：block，pool，arena 和内存池，其关系为 block -> pool -> arena -> 内存池，（pool 由多个 block 组成）
    >
    >引用计数最大的优点就是实时性，一旦没有指向它的引用，就会被立即回收。
    >
    >但引用计数机制所带来的维护引用计数的额外操作与 python 运行中所进行的内存分配与释放，引用赋值的次数是成正比的，这是一个弱点。因为主流的垃圾回收技术，比如标记-清除（mark-sweep），停止-复制（stop-copy），它们的额外操作基本上只与待回收的内存数量有关。为了与引用计数机制搭配，在内存的分配和释放上获得最高的效率，python 设计了大量的内存池机制。
    >
    >除此之外，引用计数还有一个致命的弱点：循环引用。要解决循环引用，python 引入了标记-清除和分代收集两种技术。
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/4e9279a4-f0fe-11e7-b54b-0242ac140002)
    >
    >垃圾回收一般都分为两个阶段：垃圾检测，垃圾回收。python中的标记-清除技术方案也叫三色标记模型。具体实现步骤如下：
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/19e64036-f104-11e7-958b-0242ac140002)
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/38337432-f104-11e7-b54b-0242ac140002)
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/4d856768-f105-11e7-958b-0242ac140002)
    >
    >分代回收背后的理论依据
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/ff4d149e-f111-11e7-958b-0242ac140002)
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/25cc9e78-f112-11e7-b54b-0242ac140002)
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/e4c692f2-f117-11e7-958b-0242ac140002)
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/1b4fc832-f11a-11e7-b54b-0242ac140002)
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/a907f730-f11a-11e7-b54b-0242ac140002)
    >
    >gc package 的使用可以一定程度上解决循环引用的问题。
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/3cbbabde-f11b-11e7-958b-0242ac140002)
    >
    >![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/71eae3f6-f11b-11e7-b54b-0242ac140002)

  - Python 虚拟机

    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/7289f3bc-fb23-11e7-b54b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/839c89ca-fb24-11e7-b54b-0242ac140002)

  - Python 虚拟机框架

    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/a9de7e84-fb2b-11e7-b54b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/fc41bb3c-fb2b-11e7-b54b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/7ff26d4a-fb2d-11e7-958b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/859cde00-fb2e-11e7-958b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/b5a0f546-fb2e-11e7-b54b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/6247fac4-fb2f-11e7-958b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/553c3308-fb30-11e7-958b-0242ac140002)
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/5ade690c-fb30-11e7-b54b-0242ac140002)

  - Python 虚拟机中的类机制

    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/131bec7a-fb4e-11e7-b54b-0242ac140002)

  - Python 中的 import 机制

    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/f3e16a02-fbf1-11e7-958b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/9b2abe16-fbf3-11e7-b54b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/e0a0b036-fbf3-11e7-958b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/d698aae8-fbf4-11e7-958b-0242ac140002)
    >
    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/f1e6985a-fbf9-11e7-958b-0242ac140002)

  - Python 多线程机制

    > ![图片注释](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/21eb5f8a-fbfb-11e7-958b-0242ac140002)
    >
    > ​

    ​

- ***Answering***: 嗯，有很多问题想从这本书中找到答案

  - 内存管理垃圾回收：
  - 高并发，异步，多线程多进程：
  - 动态导入：
  - 性能优化：

- ***questions:*** Any questions after reading

- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews




## 9. 2018.03 | 精进 

- ***reason***: what makes you read that book

> 一次校友 mba 分享会上推荐的书，当时推荐的还有吴敬琏老师的《当代中国经济改革教程》，看了介绍很有意思，正好也有 kindle 版的，就买来看看。当然，其实最吸引我的还是这本书的副标题：怎么成为一个很厉害的人。

- ***what***: what does that book tells, the opinion, the methodology, using the catalog

> 从作者的经历，感受和思考来说如何成为一个很厉害的人，或者如何让自己变得更厉害，里面提出的一些观点很值得参考：
>
> - 时间之尺，我们应该怎么对待时间：
>   - 在分析一件事情值不值得去做， 花多少精力去做的时候，可以从两角度来评估：一是这件事在当下 将给“我”带来的收益大小，称之为“ 收益值”；二是这项收益随时间衰减的速度，称之为“收益半衰期”，半衰期长的事件，其影响会持续较久。
>   - 找到并坚持至少一项长期的业余爱好。
> - 寻找心中的巴拿马，如何做出更好的选择
>   - 人生就是一个连点成线的过程，有些经历也许一开始看不到它们的意义所在，但也许若干年后便会发挥其特有的作用；
>   - “改造”爱好的一个常见方法就是把对一件事情的“消费型兴趣”升级为“生产型兴趣”；
>   - 所谓选择，就是要权衡好本末轻重，清楚自己的人生中到底想要什么，追求什么；
>   - 过去的经历，习惯和思维惯性，常常在我们思考时植入“隐含假设“，让我们意识不到本来就存在的其他更多选项；
> - 即刻行动，最有效的，就是即刻行动
>   - “现在”，就是最好的时机，不管怎样，只要开始就好；一件看上去繁难的事，只要开始做了，就会变得越来越容易；当不知道怎么去做一件事的时候，就直接开始做吧，只要开始了第一步，就会有第二步第三步；
>   - 精益创业三要素：克服“过度准备”，向前一步，把未完成的事情完成；克服“自我防卫”的心态，乐于接受反面意见并加以慎重的审视；克服“沉默成本”的固执，有勇气否定并重新构造自己的产品；
>   - 拿到一个任务后，务必先找到那个任务的核心思考区间，找到那块硬骨头，尽全力啃下来，而不是先去做那些周边打扫性的工作；
>   - 三思而后行，在实践中，通过复盘来积累智慧；
> - 怎样的学习，才能够直面现实，如何成为一个高段位的学习着
>   - 找到一切学习的向导，好的学习者，首先要向自己提问；
>   - 不要只做信息的搬运工，通过解码，深入事物的深层；
>   - 技能才是学习的中点，你能够调用点知识有多少？你掌握了多少知识，并不取决于你记忆了多少知识以及知识点关联，而是取决于你能够调用多少知识以及知识关联；
>   - 求知分为3个层级：信息，知识和技能；最差的学习者只接收信息，贪多求广；好一点的学习者看重知识，以记忆为目标；高手磨练技能，只求日日精进；
> - 向未知的无限逼近，修炼思维，成为真正的利器
>   - 信息过载不是因为信息太多，而是我们的“过滤器”失效了，所以简化外界输入的信息，前提是修好我们的“过滤器”。
>   - 表达本就是训练思维的一种手段，好的表达，对思考强度的要求是成倍上升的，正所谓“吟安一个字，捻断数茎须“。所以，有意识的培养自己简洁表达的习惯，是提升思维能的一个法门；
>   - 当我们思考一个现实中的场景问题时，如果很快就找到了答案，那么可能意味着这个答案并不周密，只顾及了问题中某个侧面或者局部。若真的要把一个问题所涉及的方方面面想清楚，则要困难得多，只有少数思维能力极强的人才能做到这一点。可以说，一个人思考问题的周密程度的这个人思维品质的主要指标之一。
> - 努力，是一种最需要学习的才能，不断优化你的“努力”方式
>   - 后天努力对人的改造作用是惊人的，以大多数人的努力程度之低，根本轮不上拼天赋；
>   - 没有突出的长板最危险，专注发展自己的优势才能；
>   - 基于岗位要求找出最具有相应能力的人，而忽略他在其他方面的弱点，也就是说，一个员工具备突出的优点比他没有明显的弱点要重要得多；
>   - 浅尝辄止的人很可能将一无所获，而专注投入走到最后的人将获得超额收益；
>   - 仔细考察自己的优势和劣势，利用自己的优势努力工作，通过与人合作来衡量自己的优势，回避在很多不同方向上空耗精力，保持专注，把自己能做的做到最好，并保持留意新的机会；
>   - 挑战是设计出来的，不断的为自己设计“必要的难度“挑战；学习的时候，主动的给自己增加一些难度，这对提升学习效果是非常必要的；
>   - 好的学习者，在学习别人的同时，更要懂得自己去琢磨；
>   - 每个人都有自己独特的优势，并且可以通过努力变得更加优秀；
>   - 努力不是一味的用力，而是一种具有策略性的活动，可以不断学习和优化；
> - 每一个成功者，都是唯一的，创造成功，而不是复制成功
>   - 独特性，就是最好的竞争力，请坚持你的与众不同；
>   - 战略的意义就在于让你远离竞争，战略不是要你做得更好，而是要让你做得不同。一个显而易见的事实是：如果你与大多数人相比没有什么特殊之处，那么为什么社会要给你超出平均水准的回报？
>   - 让自己变得独特是通向成功的必要条件，下面是我总结的具有心智独特性的人所具有的优势：
>     - 在主流观点之外洞察出别人未曾发现的机会；
>     - 形成个人和心竞争力，避免低层次的同质化竞争，使自己不可替代；
>     - 拒绝他人和大众给自己贴上标签，以更开放和自由的心态发展自己；
>     - 因为不必迎合社会主流而节约来大量时间精力，可以专注于做好自己的事情；
>     - 为大众带来新鲜的见解和启发，形成对公众对影响力；
>     - 具有更高的可辨识性，更易于形成个人品牌；
>     - 吸引到其他独特而优秀的人，与她们成为朋友或者合作伙伴；

- ***Answering***: Does the book answer your questions

> 其实并没有带着多少问题来读这本书，但是里面的观点很受用，特别是关于技能的半衰期这个概念。

- ***questions:*** Any questions after reading


- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews

> 穷查理宝典：查理·芒格智慧箴言录



## 10. 2018.03 | 商业创新设计

- ***reason***: what makes you read that book

> 因为书名，同时自己对商业模式很感兴趣，哈哈哈。核心原因是想如果未来自己带团队做产品，或者做pm，或者创业的时候，如果在商业上占有优势，因为自己目前在这方面积累比较少。

- ***what***: what does that book tells, the opinion, the methodology, using the catalog

> 记不清了，只能说看完第一遍没有学到什么，或者是自己还没有理清楚。

- ***Answering***: Does the book answer your questions
- ***questions:*** Any questions after reading
- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews



## 11. 2018.03 | 卓有成效的管理者

- ***reason***: what makes you read that book

> 之前看过，准备再回顾一次，总结总结，经典书籍。

- ***what***: what does that book tells, the opinion, the methodology, using the catalog

> 关于管理方面的著作通常都是谈如何管理别人的，而本书的主题却是关于如何才能使自己成为卓有成效的管理者；管理者能否管理好别人从来就没有真正验证过，但管理者却完全可以管理好自己。
>
> 这本书在讲解管理的时候，更多的思考方向是从管理者自身出发，我觉得非常好，即处理问题，反思问题的时候更多的去考虑自身因素，而不是花很多时间精力去考虑外界因素，这样的人都会很优秀，正如内因，外因的说法一样，我想德鲁克应该是更看重内因吧，所以才会这样成功。
>
> 要成为卓有成效的管理者，就要做到亲自实践。
>
> 读百本书，不如好书读百遍。
>
> - ***核心观点：传统管理者面临的问题***：
>   - 管理者的时间往往只属于别人，而不属于自己；
>   - 管理者往往被迫按照一套传统方法开展工作；
>   - 只有当别人使用管理者的贡献时，管理者才具有有效性；也就是说，只有管理者带领的团队做出对外界有益的产品出来，给公司带来正收益的情况下，这个管理者才是有效的管理者；
>   - 管理者身处组织之内，但如果他要做到有效的工作，还必须认识到组织以外的情况；
> - ***核心观点：德鲁克的有效管理者理论***
>   - 知道如何利用自己的时间；
>   - 注意使自己的努力产生必要的结果，能给公司带来正收益的结果；而不是仅仅关注工作本身；
>   - 把工作建立在优势上 —— 他们自己的优势，善于利用自己的长处，上级，同事和下属的长处；
>   - 集中精力于少数主要领域；
>   - 善于做出有效的决策；
>
>
> - 卓有成效是可以学会的 
>   - 知识工作不能用数量来衡量，也不能用成本来衡量，而是应该用结果来衡量；
>   - 每一位管理者都要面对的现实是，一方面要求他们具有有效性，一方面却又使他们很难达成有效性；
>   - 卓有成效的管理者都有一个共同点，那就是他们在实践中都要经历一段训练，这一训练使他们工作起来卓有成效；
>   - 卓有成效的管理者的思维习惯：1. 知道时间用在了哪些地方；2. 重视对外界的贡献，而不是为了工作而工作；3. 善于利用自己的长处，善于抓住形式做自己想做的事；4. 集中精力于少数重要的领域；5. 善于做有效的决策；
> - 掌握自己的时间
>   - 如果要管理好自己的时间，首先应该了解自己的时间实际上是怎么耗用的；
>   - 判断团队人数是否过多的一个方法，如果一个高级管理人员需要花 1/10 的时间处理所谓的 “人际关系问题” 上，比如处理纠纷和摩擦，处理争执和合作等问题上，那么这个单位就人数过多了，人数过多难免彼此侵犯，也难免成为绩效的障碍；在精干的组织里，人的活动空间较大，不至于互相冲突，工作时也不用每次都向别人说明。
> - 我能贡献什么
>   - 重视贡献，才能使管理者的注意力不为其本身的专长所限，不为其本身的技术所限，不为其本身所属的部门所限，才能看到整体的绩效，同时也能使他更加重视外部世界。
>   - 有效的人际关系有几项基本要求：互相沟通，团队合作，自我发展，培养他人；
> - 如何发挥人的长处
>   - 一个职位，如果先后两三个人都担任失败了，那这就是一个常人无法胜任的职位，这个职位就必须重新设计；
>   - 只有经得起绩效考验的人，才是可以提升的人，这是一条用人铁律；
>   - 任何一项人事任命都是一个赌注，但是只要能抓住某人的长处是什么，这至少是一个合理的赌注；
>   - 卓有成效的管理者还要设法充分发挥上司的长处，这也是非常重要的；
> - 要事优先
>   - 把重要的事情放在前面先做，而且一次只做好一件事情；
>   - 有些人一事无成，而事实上他们却做得很吃力；
>   - 由于搁置实际上等于被取消，所以管理者都不敢轻易的延缓任何工作；
> - 决策的要素
>   - 一个垄断性的企业虽然没有对手，但应该以将来为对手；
> - 有效的决策
> - 管理者必须卓有成效

- ***Answering***: Does the book answer your questions
- ***questions:*** Any questions after reading
- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews



## 11. 2018.03 | 

- ***reason***: what makes you read that book
- ***what***: what does that book tells, the opinion, the methodology, using the catalog
- ***Answering***: Does the book answer your questions
- ***questions:*** Any questions after reading
- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews



## 11. 2018.03 | 

- ***reason***: what makes you read that book
- ***what***: what does that book tells, the opinion, the methodology, using the catalog
- ***Answering***: Does the book answer your questions
- ***questions:*** Any questions after reading
- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews



## 11. 2018.03 | 

- ***reason***: what makes you read that book
- ***what***: what does that book tells, the opinion, the methodology, using the catalog
- ***Answering***: Does the book answer your questions
- ***questions:*** Any questions after reading
- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews



## 11. 2018.03 | 

- ***reason***: what makes you read that book
- ***what***: what does that book tells, the opinion, the methodology, using the catalog
- ***Answering***: Does the book answer your questions
- ***questions:*** Any questions after reading
- ***reviews:*** read and collect some high-quality reviews by others, best reviews and worst reviews




# 商业模式分析

## 1. 2018.03 | 当当网

- 背景

  > ​3月9日晚间，于1月12日起停牌至今，1月15日正式启动重大资产重组事项的天海投资披露重组进展，公司首度披露标的资产为北京当当科文[电子商务](https://link.zhihu.com/?target=http%3A//www.ebrun.com/)有限公司及北京当当网信息技术有限公司相关股权。
  > 天海投资拟向交易对方购买上述标的资产，将涉及发行股份购买资产，并视情况进行配套融资。本次交易完成后不会导致公司实际控制权发生变更。

- 核心论点

  - 企业创始人的眼光和创始人团队在很大程度上决定了这个企业未来的长期发展。

- 经验总结

  - 要有生态思维，生态能带来的，远远不止一个单类产品的价值，但是要注意步子也要注意投资；比如为什么京东要和当当打价格战，以及打价格战分别对两家公司的利弊；
  - 居安思危，时刻保持危机感；
  - 创业公司管理层配置；
  - 风口时代，走慢一步就是死，走得不快就被困，只有走得最快才是唯一的赢家；

- 相关链接

  - [格局决定结局，当当网做错了什么？](https://mp.weixin.qq.com/s/f3I_V0-Sv8jPG6BAfqLbIQ)
  - [当当网是如何走到被收购这一步的？](https://www.zhihu.com/question/268621461/answer/339947734)



## 2. 2018.03 | 同花顺，万得，大智慧，东方财富

- 背景

- 核心论点

- 经验总结

- 相关链接

  - [激荡二十五年：Wind、同花顺、东方财富、大智慧等金融服务商争霸史](https://awtmt.com/articles/294071)



## 3. 2018.03 | 

- 背景

  > ​

- 核心论点

- 经验总结

- 相关链接

  - link



## 1. 2018.03 | 

- 背景

  > ​

- 核心论点

- 经验总结

- 相关链接

  - link



## 1. 2018.03 | 

- 背景

  > ​

- 核心论点

- 经验总结

- 相关链接

  - link

- ​

# 高质量文章

## 1. 2017.10 | 30条社会扎心潜规则，你看懂了几条？

> 能在一定位置上的人，一定有他的过人之处，不管你多么讨厌他。
>
> 要想屏蔽某些人的朋友圈，最好把同事微信分到一个组里，要屏蔽一起都屏蔽了。
>
> 不要总在旁人面前提你的朋友多牛逼，你要懂得，别人的成就与你无关。
>
> 朋友同事之间，帮忙是情分，不帮忙是本分，不要把别人对你的好，当作理所当然。
>
> 和同事拼单买东西叫外卖，一定把支付明细的截图发给每个人，你信任我，当然我也靠得住。
>
> 借了别人的钱一定要按时还，如果一次还不了就分多次，如果没钱就多联系对方，千万别因愧疚而不理对方，很容易被理解为心安理得不闻不问。
>
> 不要轻易把心底深处不愿示人的部分坦露出来，靠同情轻易获得的朋友，内心不一定看得起你。
>
> 不管学校，职场或社会，都不要违背心意地一味讨好别人，有些圈子终究不属于自己，努力做好自己，讨好自己可能会更容易有丰厚回报。
>
> 不要总是反驳别人，更不要总在他人面前抱怨。抱怨不但解决不了问题，还会一次次地强化顽固思维，本来没多大的事儿说多了就更让自己生气，最终陷入全世界都亏欠自己的怪圈。
>
> 我们总是喜欢拿顺其自然来敷衍人生道路上的荆棘坎坷，却很少承认，真正的顺其自然，其实是竭尽所能之后的不强求，而非两手一摊的不作为。
>
> 别随性地把兴趣变成职业。
>
> 不管你是想去创业，还是想去流浪，如果你一直犹豫且小心翼翼，那是因为你还没有试错的成本。一旦决定了一件事就要踏实去做，别一鼓作气再而衰三而竭。
>
> 收到回复是一种尊重，是人品的体现。
>
> 别人给你看手机里的照片，请不要顺手左右划着看，别人东西不要随便乱动，想用先征询一下别人的意见，如果借给了你定要好好爱惜。
>
> 不要隔着屏幕就去诋毁一件事或是一个人，你所看到的，只是别人希望你看到的。事情的真相，只有当事人清楚。
>
> 人生苦短，你要把善意、耐心、尊重、宽容用在那些值得你这样对待的人身上，有些人，不值得。
>
> 工作后，你的交际圈会越来越小，很难认识新的朋友，那种没有任何功利性的朋友更是难得。
>
> 当你辉煌时，身边总是不缺朋友，而当你窘迫时，朋友却都散开了。真正的朋友是什么样子？大概就是：我成功，他不嫉妒；我萎靡，他不轻视。
>
> 真正坚持到最后的人靠的不是激情，而是恰到好处的喜欢和投入。凡事适度即可，太用力的人跑不远，太用力的爱不圆满。
>
> 看破不说破，知人不评人，知理不争论。
>
> 沉不下心看书，浮躁和焦虑，都是因为年纪渐长，不信正道而太重功利导致的。
>
> 学会存钱，克制欲望，你永远不知道什么时候要用到钱。
>
> 身体是革命的本钱。一个很差的身体带给你的局限不可能靠意志力突破。
>
> 我从来不相信什么懒洋洋的自由，我向往的自由是通过勤奋和努力实现的更广阔的人生，那样的自由才是珍贵的、有价值的；我相信一万小时定律，我从来不相信天上掉馅饼的灵感和坐等的成就。做一个自由又自律的人，靠势必实现的决心认真地活着。
>
> 便宜的东西，只有你买的那一刻是开心的，用的时候没有一天是开心的，品质好的东西，给钱那一刻是心疼的，用的时候每天都是快乐的，感觉特别值得。
>
> 我犯的最大的错误，就是我以为往下走工作比较轻松，实际上往上走反而比较轻松，因为层次高的人脑子比较清楚，做事讲道理。小地方很多人脑子转不开，你每天去跟他们吵吵，时间精力都浪费在这上面了。找对象也是一样，你以为你找一个老实的，单纯的，好过日子，实际上他脑子要是不好，更麻烦。
>
> 麦兜说：有事情是要说出来的，不要等着对方去领悟，因为对方不是你，不知道你想要什么，等到最后只能是伤心和失望，尤其是感情。
>
> 不要对一个人太好，因为你终会发现，这样时间久了，那个人是会习惯的，然后把你做的一切看作是理所应当。其实本来是可以蠢到不计代价不顾回报的，但现实总是让人寒了心。其实你明明知道，最卑贱不过感情，最凉不过是人心。
>
> 这个社会很现实：久病床前无孝子，久贫家中无贤妻。有钱走遍天下，无钱寸步难行。
>
> 人的天性本就凉薄，只要拿更好的来换，一定会舍得。



## 2. 2017.10 | 深度好文：阶级竞争即将由抢房，升级为抢

>**我们应该积极的思考：阶级竞争的本质是什么，终局是什么，下一步如何布局？**
>
>在中国，阶级竞争的焦点不会在房产停留太久，会很快向前切换，不断升级演变：
>
>1. 开局是地产（静态博弈，一劳永逸）；
>2. 中场是教育（动态博弈，价值提升）；
>3. 终局是时间（全局博弈，拿钱买命）。
>
>重视教育并不是亚洲家长的偏执，而是社会发展的必然：
>
>1. **高薪工作所需的技能和知识壁垒在不断加高**
>2. **技术的进步在加速阶层的洗牌和分化，高知阶层碾压底层是常态**
>3. **保持足够强的学习能力是保持在本阶层的关键。**
>
>**智商税是这个地球上最重的赋税。**

## 3. 2017.10 | 分分钟读懂私募股权投资“募、投、管、退”全流程！

> GP: 普通合伙人，有限合伙制基金中承担基金管理人角色的投资管理机构。
>
> LP: 有限合伙人，有限合伙制基金中的投资者。
>
> **天使投资：**是权益资本投资的一种形式，指富有的个人出资协助具有专门技术或独特概念的原创项目或小型初创企业，进行一次性的前期投资。
>
> **VC（VentureCapital,风险投资）：******由风险投资机构投入到新兴的、迅速发展的、具有巨大竞争潜力的企业中的一种权益资本，即对成长期企业的投资。
>
> **PE（PrivateEquity,私募股权投资）：******与上述VC的定义对比来讲，此处指狭义的私募股权投资。狭义的PE主要指对已经形成一定规模的，并产生稳定现金流的成熟企业的私募股权投资。而广义的PE指涵盖企业首次公开发行前各阶段的权益投资，即处于种子期、初创期、发展期、扩展期、成熟期和Pre-IPO各个时期企业所进行的投资。主要可以分为三种：PE-Growth：投资扩张期及成熟期企业；PE-PIPE：投资已上市企业；PE-Buyout：企业并购，欧美许多著名私募股权基金公司主要业务。
>
> **PE FOFs：******私募股权母基金。
>
> **承诺出资制：******承诺出资是有限合伙形式基金的特点之一，在资金筹集的过程中，普通合伙人会要求首次成立时一定比例的投资本金到位，而在后续的基金运作中，投资管理人根据项目进度的需要，以电话或者其他形式通知有限合伙人认缴剩余部分本金。
>
> **优先收益：******又称“门槛收益率”，优先收益条款确保了一般合伙人只有在基金投资表现优良之时才能从投资收益中获取一定比例的回报。
>
> **IPO（InitialPublic Offerings,首次公开募股）**
>
> **并购：******一般指兼并和收购。
>
> **尽职调查：******在基金或公司层面对私募股权进行成功投资需要事先详尽的调查。要进行长期投资，有必要在签约前审核分析交易牵涉的所有因素。尽职调查要全面审核许多因素，比如管理团队的能力、公司业绩、交易状况、投资战略、合法证券等等。
>
> **关键人条款：******由于私募股权基金投资风险较大，基金管理人的能力对于基金整体业绩的影响较大，因而基金条款中会有关键人条款，即基金团队中指定的核心成员在整个基金存续期间不得离开，否则投资人有权要求召开合伙人会议，提前对基金进行清算。
>
> **联合投资：******对于一个投资项目，可能会有多个机构同时关注。
>
> **过桥融资贷款************（Bridgefinancing）：******公司在正式IPO或私募融资前为确保经营和融资活动顺利推进所进行的短期贷款，通常期限在1年以内，最终会被永久性的资本如股权投资者或长期债务人所取代。
>
> PE基金的运作流程可以细分为四个环节：募资、投资、管理、退出。
>
> ![](https://mmbiz.qpic.cn/mmbiz_jpg/TF5yroGAEibbcHZa76YTry2tAEFCW2gV9MkA7tE6RFYlEjFaRVkicpkYES2wZsiblEO9y5EXeWGInXvMS5XevZNGw/640)



## 4. 2017.10 | Lambda架构实践-打造高并发实时计量的智能数据商场

> ![](https://mmbiz.qpic.cn/mmbiz_png/8biaFoWLFLS0gKiaKXQjmF1VTDOMDeOia16yic8vic6wTJZMq3t3wPh93qibKWzoozXibrO5PbR9JneJXat61ic9ibXwCAw/640)
>
> 这是一个典型的 Lambda 架构系统。对用量的计算分为两个部分，批处理部分（批处理层）和实时流处理部分（速度层），最后由查询服务（服务层）聚合批处理层和快速层的数据提供查询服务。
>
> - lambda 架构
>   - 用函数式编程的观点来设计系统架构，在宏观层面上享受了一些函数式编程的优势
>   - 大数据处理技术需要解决这种可伸缩性与复杂性。首先要认识到这种分布式的本质，要很好地处理分区与复制，不会导致错误分区引起查询失败，而是要将这些逻辑内化到数据库中。当需要扩展系统时，可以非常方便地增加节点，系统也能够针对新节点进行rebalance。其次是要让数据成为不可变的。原始数据永远都不能被修改，这样即使犯了错误，写了错误数据，原来好的数据并不会受到破坏。
>   - Storm的作者NathanMarz提出的一个实时大数据处理框架（Lambda架构）就满足以上两点。Marz在Twitter工作期间开发了著名的实时大数据处理框架Storm，Lambda架构是其根据多年进行分布式大数据系统的经验总结提炼而成。
>   - Lambda架构的目标是设计出一个能满足实时大数据系统关键特性的架构，包括有：高容错、低延时和可扩展等。Lambda架构整合离线计算和实时计算，融合不可变性（Immunability），读写分离和复杂性隔离等一系列架构原则，可集成Hadoop，Kafka，Storm，Spark，Hbase等各类大数据组件。

## 5. 2017.11 |刚刚，吴恩达讲了干货满满的一节全新AI课，全程手写板书 

> https://www.youtube.com/watch?v=mQ29kb_jZYI 原版视频，全程手写板书，确实功力深厚，对这个产业也比较有经验。其中非常强调数据的重要性，确实如此，没有大量的高质量的数据做支撑，ai 根本做不起来；其次其对互联网公司，人工智能公司的定义也很有借鉴意义。

## 6. 2017. 11 | 无数人赞叹过的民间神文案

> 好久之前就想收集这些创意的文案了，以后开公司肯定会用到。https://mp.weixin.qq.com/s/KHjqBfUpb7YgPn0br7UpQg

## 7. 2017.11 | 创始人离奇被捕，深圳赛龙突然死亡之谜

> 原来的微信文章被删掉了，找到了这篇钛媒体的：https://weibo.com/ttarticle/p/show?id=2309614168541275304447。整件事情的核心问题，估计就是官商利益分配不和导致的吧。文章很长，有空可以读读，看完对我的感触有几点：
>
> - 创业维艰，守富更难；钱越多越要防范风险，否则有可能一夜之间变得一穷二白了；
> - 公司稳定发展后，对新业务的投资一定要有度，不要掉入黑底洞了，比如最近一两年乐视的事，也是够操蛋的；
> - 找合作方一定一定要谨慎再谨慎，特别是跟zf合作；
> - 任何时候，一定要有自己的安全垫，即使别人看得见的财富已经game over了，也要有“私房钱”啊；

## 8. 2017.11 | 创业 3 年半，我积累了这 10 条经验

> 原文：https://blog.ycombinator.com/the-most-important-decision-is-getting-started-laura-behrens-wu/
>
> 译文：http://app.myzaker.com/news/article.php?app_id=562&pk=5a0510991bc8e02850000029&sharechannel=wx
>
> - **真正决定启动创业项目是一个创始人能做的最重要的决策。**
> - **选择创业项目时，要选择做 " 止痛药 "，而不是 " 维他命 "。**
> - **找融资时，要做好被拒绝的准备。**
> - **与潜在投资人之间建立信任非常重要。**
> - **找到对公司而言最重要的一个北极星般的指标。**
> - **创业中总会出现各种突发问题，你要做的是学会妥善应对。**
> - **创业是一场马拉松，而不是一场百米冲刺。**
> - **对自己要充满自信，要做真实的自己，不要对你真实的自己有任何歉意**
> - **创业很累，要学会照顾好自己。**
> - **选对创业合伙人太重要了。**

## 9. 2017.11 | 细思极恐的YouTube可跳过广告

> 原文：https://media.weibo.cn/article?id=2309404173389983009423&jumpfrom=weibocom&from=timeline&isappinstalled=0
>
> 这是一篇新浪微博产品经理针对youtube视频可跳过广告的专业性分析，首先文章质量很高，分析过程很值得学习。看了这篇文章突然让我觉得真正的高级产品经理真的很厉害，自己也需要加强这方面的培养，以后创业肯定会帮助巨大。然后怎么去培养这方面的能力呢？我觉得可以通过多读，多思考，多验证来培养。比如说多读这种非常专业细致的分析文章，争取把它们变成自己的东西；其次，见到新奇的东西，自己要去思考为什么那些东西会那要做，然后去找一些专业的文章，专业的人士交流来验证，看看自己在分析的过程中有哪些做得好的哪些做得欠缺的。
>
> anyway，关于创业这件事，我觉得是一个水到渠成的事情，只要不间断的学习积累，时机成熟时成功自然而然就来到来了。



- ​
- ​
- [一直以来伴随我的一些学习习惯(三)：阅读方法]([http://mindhacks.cn/2008/09/17/learning-habits-part3/)
- ​